{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_final",
      "provenance": [],
      "mount_file_id": "https://github.com/anosharahim/deep-learning/blob/master/DL_final.ipynb",
      "authorship_tag": "ABX9TyOKiUiiaHQEgjzHwDhFY5fH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anosharahim/deep-learning/blob/master/DL_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and Libraries"
      ],
      "metadata": {
        "id": "zrSaV5p2_3t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import math \n",
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model \n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, ZeroPadding2D\n",
        "from keras.layers import Dropout, Activation, Flatten, GlobalAveragePooling2D \n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau \n"
      ],
      "metadata": {
        "id": "G6xdo2BhxCnN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "oiq9fqNQ_2Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data from gdrive\n",
        "train_npz = np.load(\"/content/drive/MyDrive/Capstone/datasets/train.npz\")\n",
        "test_npz = np.load(\"/content/drive/MyDrive/Capstone/datasets/test.npz\")\n",
        "\n",
        "x_train = train_npz[\"arr_0\"]\n",
        "y_train = train_npz[\"arr_1\"]\n",
        "x_test = test_npz[\"arr_0\"]\n",
        "y_test = test_npz[\"arr_1\"]\n",
        "\n",
        "emotion_dictionary = {'angry':0, 'disgusted':1, 'fearful':2, 'happy':3, 'neutral':4, 'sad':5, 'surprised':6}\n",
        "\n",
        "#randomly undersample majority class \n",
        "class_undersample =  {0: 3995, 1: 436, 2: 4097, 3: 5500, 4: 4965, 5: 4830, 6: 3171}\n",
        "x_train_reshape = x_train.reshape(len(x_train),48*48)\n",
        "undersample = RandomUnderSampler(sampling_strategy=class_undersample)\n",
        "x_train_under, y_train_under = undersample.fit_resample(x_train_reshape,y_train)\n",
        "\n",
        "#oversample minority class  using k-nearest data augmentation\n",
        "class_oversample =  {0: 3995, 1: 3500, 2: 4097, 3: 5500, 4: 4965, 5: 4830, 6: 3171}\n",
        "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "x_train, y_train = sm.fit_resample(x_train_under,y_train_under)\n",
        "\n",
        "#visualize resampled class distribution \n",
        "values, counts_ = np.unique(y_train, return_counts=True)\n",
        "plt.title(\"Emotion Class Distribution in Augmented Training Data\")\n",
        "plt.bar(emotion_dictionary.keys(),counts_, color = 'pink')\n",
        "\n",
        "#add 2 more channels to grayscale image to imitate rgb\n",
        "x_train = np.repeat(x_train[..., np.newaxis], 3, -1)\n",
        "x_test = np.repeat(x_test[..., np.newaxis], 3, -1)\n",
        "\n",
        "#split test set into validation and test set \n",
        "x_val, x_test, y_val ,y_test = train_test_split(x_test,y_test, test_size=0.3)\n",
        "\n",
        "#resize to make sure input is the correct shape\n",
        "x_train = np.resize(x_train, (len(x_train), 48,48,3))\n",
        "x_val = np.resize(x_val, (len(x_val), 48,48,3))\n",
        "x_test = np.resize(x_test, (len(x_test), 48,48,3))\n",
        "\n",
        "print('Train -- ', x_train.shape, y_train.shape)\n",
        "print('Val -- ', x_val.shape, y_val.shape)\n",
        "print('Test -- ', x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "UzbFTyF9xTcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "4993bf36-ebbb-4c05-f854-3505b3ddb2ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train --  (32058, 48, 48, 3) (32058,)\n",
            "Val --  (5024, 48, 48, 3) (5024,)\n",
            "Test --  (2154, 48, 48, 3) (2154,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "snUQJAx8P2Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Model "
      ],
      "metadata": {
        "id": "MTGxYpJ2_7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG16(weights = 'imagenet', include_top=False, input_shape= (48,48,3))\n",
        "n_freeze = 19\n",
        "for layer in vgg.layers[:n_freeze]:\n",
        "  layer.trainable=False\n",
        "\n",
        "for (i,layer) in enumerate(vgg.layers):\n",
        "  print(str(i)+' '+ layer.__class__.__name__, layer.trainable)\n",
        "\n",
        "def fully_connected(base_model, num_classes):\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dense(num_classes, activation='softmax')(x)\n",
        "  return x\n",
        "\n",
        "num_class = 7\n",
        "FC_head = fully_connected(vgg, num_class)\n",
        "model = Model(inputs=vgg.input, outputs = FC_head)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWjmwm1n8970",
        "outputId": "722f675d-be98-46fe-e779-59ff0d750d37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 48, 48, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 48, 48, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d_5   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,877,831\n",
            "Trainable params: 3,158,023\n",
            "Non-trainable params: 14,719,808\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training \n",
        "\n",
        "The good thing is that the model is working in a reasonable manner. The bad thing is that the model is overfitting on the training data. What can we do? \n",
        "\n",
        "Training accuracy was about 91% and test/val accuracy was pleatauing at about 42%\n",
        "\n",
        "Added 30% dropout layers where overfitting decreases but performance still plateaued at about ~40 for validation set\n",
        "\n",
        "- Added Dropout \n",
        "- Used global average pooling: layer helps aggregate feature map information and helps prevent overfitting caused by fully connected layers in the\n",
        "model\n",
        "- Tried all three different optimizers \n",
        "- Data augmentation --- but Image data generator eta is in 4 hours and more\n",
        "- Tried using balanced vs imbalanced training data but no change  \n",
        "- Try using unfrozen layers [but when i do this loss becomes NaN]\n",
        "- Tried adding batch norm 2% improvement\n"
      ],
      "metadata": {
        "id": "63932i95_-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bs = 32\n",
        "rms_lr = 0.001\n",
        "sgd_lr = 0.01\n",
        "adam_lr = 0.001\n",
        "sgd_decay = 0.0001\n",
        "\n",
        "lrd = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
        "es = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, verbose = 1, restore_best_weights = True)\n",
        "checkpoint = ModelCheckpoint('face_vgg.h5', monitor = 'val_accuracy', mode ='max', save_best_only = True, verbose=1)\n",
        "callbacks = [checkpoint, es, lrd]\n",
        "\n",
        "\n",
        "sgd = SGD(learning_rate=sgd_lr, momentum=0.9, decay=sgd_decay, nesterov=True) \n",
        "adam = Adam(learning_rate=adam_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "rms = RMSprop(learning_rate=rms_lr)\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=bs, \n",
        "                    epochs=epochs, \n",
        "                    verbose=1,\n",
        "                    validation_data= (x_val, y_val),\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-fFpOXa8_zB",
        "outputId": "f5978c9e-4cde-4fa0-d920-fe9febb6d84c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1000/1002 [============================>.] - ETA: 0s - loss: 1.6454 - accuracy: 0.3848\n",
            "Epoch 1: val_accuracy improved from -inf to 0.36843, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 17ms/step - loss: 1.6452 - accuracy: 0.3849 - val_loss: 1.6348 - val_accuracy: 0.3684 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "1000/1002 [============================>.] - ETA: 0s - loss: 1.4640 - accuracy: 0.4407\n",
            "Epoch 2: val_accuracy improved from 0.36843 to 0.37301, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 16ms/step - loss: 1.4643 - accuracy: 0.4406 - val_loss: 1.6019 - val_accuracy: 0.3730 - lr: 0.0100\n",
            "Epoch 3/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 1.3861 - accuracy: 0.4728\n",
            "Epoch 3: val_accuracy improved from 0.37301 to 0.38037, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.3862 - accuracy: 0.4728 - val_loss: 1.5949 - val_accuracy: 0.3804 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 1.3209 - accuracy: 0.4988\n",
            "Epoch 4: val_accuracy improved from 0.38037 to 0.40366, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.3209 - accuracy: 0.4988 - val_loss: 1.6008 - val_accuracy: 0.4037 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 1.2641 - accuracy: 0.5186\n",
            "Epoch 5: val_accuracy improved from 0.40366 to 0.42675, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.2641 - accuracy: 0.5186 - val_loss: 1.5433 - val_accuracy: 0.4268 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "1000/1002 [============================>.] - ETA: 0s - loss: 1.2129 - accuracy: 0.5375\n",
            "Epoch 6: val_accuracy improved from 0.42675 to 0.43073, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.2130 - accuracy: 0.5374 - val_loss: 1.5556 - val_accuracy: 0.4307 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "1000/1002 [============================>.] - ETA: 0s - loss: 1.1567 - accuracy: 0.5622\n",
            "Epoch 7: val_accuracy improved from 0.43073 to 0.43412, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.1568 - accuracy: 0.5621 - val_loss: 1.6199 - val_accuracy: 0.4341 - lr: 0.0100\n",
            "Epoch 8/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 1.1139 - accuracy: 0.5801\n",
            "Epoch 8: val_accuracy did not improve from 0.43412\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.1144 - accuracy: 0.5799 - val_loss: 1.6647 - val_accuracy: 0.4321 - lr: 0.0100\n",
            "Epoch 9/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 1.0617 - accuracy: 0.5993\n",
            "Epoch 9: val_accuracy did not improve from 0.43412\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.0615 - accuracy: 0.5994 - val_loss: 1.6577 - val_accuracy: 0.4307 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 1.0148 - accuracy: 0.6172\n",
            "Epoch 10: val_accuracy improved from 0.43412 to 0.43969, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 1.0148 - accuracy: 0.6171 - val_loss: 1.6507 - val_accuracy: 0.4397 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.9627 - accuracy: 0.6391\n",
            "Epoch 11: val_accuracy did not improve from 0.43969\n",
            "1002/1002 [==============================] - 17s 16ms/step - loss: 0.9628 - accuracy: 0.6390 - val_loss: 1.7305 - val_accuracy: 0.4331 - lr: 0.0100\n",
            "Epoch 12/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.9355 - accuracy: 0.6497\n",
            "Epoch 12: val_accuracy improved from 0.43969 to 0.44148, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.9354 - accuracy: 0.6497 - val_loss: 1.6976 - val_accuracy: 0.4415 - lr: 0.0100\n",
            "Epoch 13/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.8866 - accuracy: 0.6707\n",
            "Epoch 13: val_accuracy did not improve from 0.44148\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.8861 - accuracy: 0.6710 - val_loss: 1.7828 - val_accuracy: 0.4403 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.8426 - accuracy: 0.6869\n",
            "Epoch 14: val_accuracy did not improve from 0.44148\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.8429 - accuracy: 0.6868 - val_loss: 1.8199 - val_accuracy: 0.4369 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "1000/1002 [============================>.] - ETA: 0s - loss: 0.8075 - accuracy: 0.7002\n",
            "Epoch 15: val_accuracy improved from 0.44148 to 0.44745, saving model to face_vgg.h5\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.8076 - accuracy: 0.7003 - val_loss: 1.8937 - val_accuracy: 0.4475 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.7443\n",
            "Epoch 16: val_accuracy improved from 0.44745 to 0.45482, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.6953 - accuracy: 0.7443 - val_loss: 1.9103 - val_accuracy: 0.4548 - lr: 1.0000e-03\n",
            "Epoch 17/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.6587 - accuracy: 0.7597\n",
            "Epoch 17: val_accuracy did not improve from 0.45482\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.6587 - accuracy: 0.7597 - val_loss: 1.9538 - val_accuracy: 0.4518 - lr: 1.0000e-03\n",
            "Epoch 18/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.6353 - accuracy: 0.7656\n",
            "Epoch 18: val_accuracy did not improve from 0.45482\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.6353 - accuracy: 0.7656 - val_loss: 1.9682 - val_accuracy: 0.4548 - lr: 1.0000e-03\n",
            "Epoch 19/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.6220 - accuracy: 0.7701\n",
            "Epoch 19: val_accuracy improved from 0.45482 to 0.45701, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.6220 - accuracy: 0.7702 - val_loss: 2.0140 - val_accuracy: 0.4570 - lr: 1.0000e-03\n",
            "Epoch 20/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.6121 - accuracy: 0.7754\n",
            "Epoch 20: val_accuracy improved from 0.45701 to 0.45780, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.6121 - accuracy: 0.7754 - val_loss: 2.0412 - val_accuracy: 0.4578 - lr: 1.0000e-03\n",
            "Epoch 21/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.7812\n",
            "Epoch 21: val_accuracy improved from 0.45780 to 0.45820, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.5957 - accuracy: 0.7812 - val_loss: 2.0596 - val_accuracy: 0.4582 - lr: 1.0000e-03\n",
            "Epoch 22/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.5886 - accuracy: 0.7838\n",
            "Epoch 22: val_accuracy improved from 0.45820 to 0.46218, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.5885 - accuracy: 0.7838 - val_loss: 2.0660 - val_accuracy: 0.4622 - lr: 1.0000e-03\n",
            "Epoch 23/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.5767 - accuracy: 0.7881\n",
            "Epoch 23: val_accuracy improved from 0.46218 to 0.46318, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5768 - accuracy: 0.7881 - val_loss: 2.0961 - val_accuracy: 0.4632 - lr: 1.0000e-03\n",
            "Epoch 24/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.5723 - accuracy: 0.7897\n",
            "Epoch 24: val_accuracy improved from 0.46318 to 0.46596, saving model to face_vgg.h5\n",
            "1002/1002 [==============================] - 18s 18ms/step - loss: 0.5723 - accuracy: 0.7897 - val_loss: 2.1051 - val_accuracy: 0.4660 - lr: 1.0000e-03\n",
            "Epoch 25/100\n",
            "1000/1002 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7912\n",
            "Epoch 25: val_accuracy did not improve from 0.46596\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5679 - accuracy: 0.7914 - val_loss: 2.0591 - val_accuracy: 0.4612 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.7972\n",
            "Epoch 26: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5531 - accuracy: 0.7972 - val_loss: 2.0937 - val_accuracy: 0.4618 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.5521 - accuracy: 0.7983\n",
            "Epoch 27: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5521 - accuracy: 0.7983 - val_loss: 2.1026 - val_accuracy: 0.4634 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.5514 - accuracy: 0.7987\n",
            "Epoch 28: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5513 - accuracy: 0.7987 - val_loss: 2.1202 - val_accuracy: 0.4638 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.5539 - accuracy: 0.7960\n",
            "Epoch 29: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5539 - accuracy: 0.7960 - val_loss: 2.1040 - val_accuracy: 0.4646 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.5417 - accuracy: 0.7989\n",
            "Epoch 30: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5415 - accuracy: 0.7990 - val_loss: 2.0896 - val_accuracy: 0.4614 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.7997\n",
            "Epoch 31: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5494 - accuracy: 0.7997 - val_loss: 2.0909 - val_accuracy: 0.4626 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "1002/1002 [==============================] - ETA: 0s - loss: 0.5473 - accuracy: 0.7994\n",
            "Epoch 32: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5473 - accuracy: 0.7994 - val_loss: 2.1048 - val_accuracy: 0.4650 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            " 999/1002 [============================>.] - ETA: 0s - loss: 0.5520 - accuracy: 0.7956\n",
            "Epoch 33: val_accuracy did not improve from 0.46596\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5524 - accuracy: 0.7954 - val_loss: 2.1063 - val_accuracy: 0.4642 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "1001/1002 [============================>.] - ETA: 0s - loss: 0.5471 - accuracy: 0.8015\n",
            "Epoch 34: val_accuracy did not improve from 0.46596\n",
            "Restoring model weights from the end of the best epoch: 24.\n",
            "1002/1002 [==============================] - 17s 17ms/step - loss: 0.5471 - accuracy: 0.8015 - val_loss: 2.0907 - val_accuracy: 0.4618 - lr: 1.0000e-04\n",
            "Epoch 34: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8tv4YHkAUrh",
        "outputId": "d7068eeb-a2b9-4289-9dde-ea33370ef1fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 1s 17ms/step - loss: 2.1396 - accuracy: 0.4689\n",
            "Restored model, accuracy: 46.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Make a long story ---- \n",
        "show insight about everything you tried in a strategic manner. //\n",
        "Reading literature -- talk about SOTA //\n",
        "give a strong story -- //\n",
        "this is why it is an extension \n",
        "smaller network and why this is valuable \n",
        "talk about costs etc...\n",
        "Two types of engineer -- what is the most amazing vs what is the cheapest engineer //// \n",
        "we are looking for arch with small no. of trainable parameters \n",
        "easier to train with smaller dataset\n",
        "\n",
        "\n",
        "## Questions\n",
        "\n",
        "- Why is validation accuracy greater than training accuracy after adding dropout? \n",
        "- Why does my val loss doesn't decrease but val accuracy increases?\n",
        "\n",
        "## Notes\n",
        "\n",
        "**Momentum**\n",
        "\n",
        "momentum: that accelerates gradient descent in the relevant direction and dampens oscillations Momentum speeds up movement along directions of strong improvement (loss decrease) and also helps the network avoid local minima.\n",
        "\n",
        "**Nesterov momentum**\n",
        "\n",
        "nesterov momentum is a simple change to normal momentum. Here the gradient term is not computed from the current position\n",
        "while gradient may point in right direction, momentum may not always, so the intermediate position is found to redirect momentum \n",
        "\n",
        "## References \n",
        "\n",
        "1. https://dominikschmidt.xyz/nesterov-momentum/\n",
        "2. https://medium.com/@heba.elshatoury/predicting-emotion-from-facial-expression-88acc43f96f5"
      ],
      "metadata": {
        "id": "EooDMtsmBvNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sFQSLEaYCA20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}