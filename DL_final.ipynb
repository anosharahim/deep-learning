{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_final",
      "provenance": [],
      "mount_file_id": "https://github.com/anosharahim/deep-learning/blob/master/DL_final.ipynb",
      "authorship_tag": "ABX9TyPR+1Klc83l7ZAOJ+F0oOEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anosharahim/deep-learning/blob/master/DL_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and Libraries"
      ],
      "metadata": {
        "id": "zrSaV5p2_3t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import math \n",
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model \n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, ZeroPadding2D\n",
        "from keras.layers import Dropout, Activation, Flatten, GlobalAveragePooling2D \n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau \n"
      ],
      "metadata": {
        "id": "G6xdo2BhxCnN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "oiq9fqNQ_2Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data from gdrive\n",
        "train_npz = np.load(\"/content/drive/MyDrive/Capstone/datasets/train.npz\")\n",
        "test_npz = np.load(\"/content/drive/MyDrive/Capstone/datasets/test.npz\")\n",
        "\n",
        "x_train = train_npz[\"arr_0\"]\n",
        "y_train = train_npz[\"arr_1\"]\n",
        "x_test = test_npz[\"arr_0\"]\n",
        "y_test = test_npz[\"arr_1\"]\n",
        "\n",
        "#add 2 more channels to grayscale image to imitate rgb\n",
        "x_train = np.repeat(x_train[..., np.newaxis], 3, -1)\n",
        "x_test = np.repeat(x_test[..., np.newaxis], 3, -1)\n",
        "\n",
        "#split test set into validation and test set \n",
        "x_val, x_test, y_val ,y_test = train_test_split(x_test,y_test, test_size=0.3)\n",
        "\n",
        "print('Train -- ', x_train.shape, y_train.shape)\n",
        "print('Val -- ', x_val.shape, y_val.shape)\n",
        "print('Test -- ', x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "UzbFTyF9xTcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4c62e7-c67c-4ead-8c8b-0a9bbb6f0a05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train --  (28709, 48, 48, 3) (28709,)\n",
            "Val --  (5024, 48, 48, 3) (5024,)\n",
            "Test --  (2154, 48, 48, 3) (2154,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Model "
      ],
      "metadata": {
        "id": "MTGxYpJ2_7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG16(weights = 'imagenet', include_top=False, input_shape= (48,48,3))\n",
        "n_freeze = 19\n",
        "for layer in vgg.layers[:n_freeze]:\n",
        "  layer.trainable=False\n",
        "\n",
        "for (i,layer) in enumerate(vgg.layers):\n",
        "  print(str(i)+' '+ layer.__class__.__name__, layer.trainable)\n",
        "\n",
        "def fully_connected(base_model, num_classes):\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dense(num_classes, activation='softmax')(x)\n",
        "  return x\n",
        "\n",
        "num_class = 7\n",
        "FC_head = fully_connected(vgg, num_class)\n",
        "model = Model(inputs=vgg.input, outputs = FC_head)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWjmwm1n8970",
        "outputId": "a12d35e7-a0cf-4e33-db60-e899663d584f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 48, 48, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 48, 48, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d_2   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 512)              2048      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1024)              525312    \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,877,831\n",
            "Trainable params: 3,158,023\n",
            "Non-trainable params: 14,719,808\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training \n",
        "\n",
        "The good thing is that the model is working in a reasonable manner. The bad thing is that the model is overfitting on the training data. What can we do? \n",
        "\n",
        "Training accuracy was about 91% and test/val accuracy was pleatauing at about 42%\n",
        "\n",
        "Added 30% dropout layers where overfitting decreases but performance still plateaued at about ~40 for validation set\n",
        "\n",
        "- Added Dropout \n",
        "- Used global average pooling: layer helps aggregate feature map information and helps prevent overfitting caused by fully connected layers in the\n",
        "model\n",
        "- Tried all three different optimizers \n",
        "- Image data generator eta is in 4 hours and more\n",
        "- Try using balanced training data. \n",
        "- Try using unfrozen layers [but when i do this loss becomes NaN\n",
        "- Try adding batch norm "
      ],
      "metadata": {
        "id": "63932i95_-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bs = 32\n",
        "rms_lr = 0.001\n",
        "sgd_lr = 0.01\n",
        "adam_lr = 0.001\n",
        "sgd_decay = 0.0001\n",
        "\n",
        "lrd = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
        "es = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 10, verbose = 1, restore_best_weights = True)\n",
        "checkpoint = ModelCheckpoint('face_vgg.h5', monitor = 'val_accuracy', mode ='max', save_best_only = True, verbose=1)\n",
        "callbacks = [checkpoint, es, lrd]\n",
        "\n",
        "\n",
        "sgd = SGD(learning_rate=sgd_lr, momentum=0.9, decay=sgd_decay, nesterov=True) \n",
        "adam = Adam(learning_rate=adam_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "rms = RMSprop(learning_rate=rms_lr)\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=bs, \n",
        "                    epochs=epochs, \n",
        "                    verbose=1,\n",
        "                    validation_data= (x_val, y_val),\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-fFpOXa8_zB",
        "outputId": "3bf51499-971c-40f7-85d4-696da752bcbb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.8028 - accuracy: 0.3146\n",
            "Epoch 1: val_loss improved from inf to 1.65187, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 34s 17ms/step - loss: 1.8026 - accuracy: 0.3146 - val_loss: 1.6519 - val_accuracy: 0.3356 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.6615 - accuracy: 0.3559\n",
            "Epoch 2: val_loss improved from 1.65187 to 1.57904, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.6616 - accuracy: 0.3559 - val_loss: 1.5790 - val_accuracy: 0.3768 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.5952 - accuracy: 0.3819\n",
            "Epoch 3: val_loss improved from 1.57904 to 1.54936, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.5951 - accuracy: 0.3818 - val_loss: 1.5494 - val_accuracy: 0.4013 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.5506 - accuracy: 0.3958\n",
            "Epoch 4: val_loss did not improve from 1.54936\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.5507 - accuracy: 0.3957 - val_loss: 1.5581 - val_accuracy: 0.3957 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.5147 - accuracy: 0.4161\n",
            "Epoch 5: val_loss improved from 1.54936 to 1.52006, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.5147 - accuracy: 0.4162 - val_loss: 1.5201 - val_accuracy: 0.4122 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.4934 - accuracy: 0.4236\n",
            "Epoch 6: val_loss improved from 1.52006 to 1.50162, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 17ms/step - loss: 1.4934 - accuracy: 0.4236 - val_loss: 1.5016 - val_accuracy: 0.4240 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.4705 - accuracy: 0.4335\n",
            "Epoch 7: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.4705 - accuracy: 0.4335 - val_loss: 1.5049 - val_accuracy: 0.4281 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.4409 - accuracy: 0.4463\n",
            "Epoch 8: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.4411 - accuracy: 0.4461 - val_loss: 1.5099 - val_accuracy: 0.4377 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.4217 - accuracy: 0.4557\n",
            "Epoch 9: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.4217 - accuracy: 0.4557 - val_loss: 1.5150 - val_accuracy: 0.4429 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.3968 - accuracy: 0.4664\n",
            "Epoch 10: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.3971 - accuracy: 0.4663 - val_loss: 1.5273 - val_accuracy: 0.4457 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.3717 - accuracy: 0.4762\n",
            "Epoch 11: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.3717 - accuracy: 0.4762 - val_loss: 1.5289 - val_accuracy: 0.4349 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.3570 - accuracy: 0.4834\n",
            "Epoch 12: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.3572 - accuracy: 0.4833 - val_loss: 1.5107 - val_accuracy: 0.4498 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.3276 - accuracy: 0.4909\n",
            "Epoch 13: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.3278 - accuracy: 0.4909 - val_loss: 1.5113 - val_accuracy: 0.4520 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.3121 - accuracy: 0.5014\n",
            "Epoch 14: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.3121 - accuracy: 0.5014 - val_loss: 1.5517 - val_accuracy: 0.4439 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.2856 - accuracy: 0.5095\n",
            "Epoch 15: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.2858 - accuracy: 0.5095 - val_loss: 1.5098 - val_accuracy: 0.4498 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.2626 - accuracy: 0.5211\n",
            "Epoch 16: val_loss did not improve from 1.50162\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.2626 - accuracy: 0.5211 - val_loss: 1.5565 - val_accuracy: 0.4558 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.2092 - accuracy: 0.5405\n",
            "Epoch 17: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.2092 - accuracy: 0.5405 - val_loss: 1.5588 - val_accuracy: 0.4596 - lr: 1.0000e-03\n",
            "Epoch 18/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.1909 - accuracy: 0.5503\n",
            "Epoch 18: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1909 - accuracy: 0.5503 - val_loss: 1.5720 - val_accuracy: 0.4582 - lr: 1.0000e-03\n",
            "Epoch 19/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1805 - accuracy: 0.5542\n",
            "Epoch 19: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1810 - accuracy: 0.5539 - val_loss: 1.5712 - val_accuracy: 0.4590 - lr: 1.0000e-03\n",
            "Epoch 20/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.1770 - accuracy: 0.5567\n",
            "Epoch 20: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1770 - accuracy: 0.5567 - val_loss: 1.5887 - val_accuracy: 0.4574 - lr: 1.0000e-03\n",
            "Epoch 21/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.1670 - accuracy: 0.5566\n",
            "Epoch 21: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1670 - accuracy: 0.5566 - val_loss: 1.5975 - val_accuracy: 0.4614 - lr: 1.0000e-03\n",
            "Epoch 22/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.1543 - accuracy: 0.5642\n",
            "Epoch 22: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1544 - accuracy: 0.5642 - val_loss: 1.6200 - val_accuracy: 0.4590 - lr: 1.0000e-03\n",
            "Epoch 23/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1515 - accuracy: 0.5642\n",
            "Epoch 23: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1519 - accuracy: 0.5641 - val_loss: 1.5890 - val_accuracy: 0.4590 - lr: 1.0000e-03\n",
            "Epoch 24/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1546 - accuracy: 0.5669\n",
            "Epoch 24: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1543 - accuracy: 0.5670 - val_loss: 1.5966 - val_accuracy: 0.4610 - lr: 1.0000e-03\n",
            "Epoch 25/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.1359 - accuracy: 0.5728\n",
            "Epoch 25: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1359 - accuracy: 0.5728 - val_loss: 1.6142 - val_accuracy: 0.4584 - lr: 1.0000e-03\n",
            "Epoch 26/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1375 - accuracy: 0.5728\n",
            "Epoch 26: val_loss did not improve from 1.50162\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1379 - accuracy: 0.5727 - val_loss: 1.6267 - val_accuracy: 0.4590 - lr: 1.0000e-03\n",
            "Epoch 27/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1287 - accuracy: 0.5760\n",
            "Epoch 27: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1284 - accuracy: 0.5761 - val_loss: 1.6199 - val_accuracy: 0.4608 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1300 - accuracy: 0.5744\n",
            "Epoch 28: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1297 - accuracy: 0.5745 - val_loss: 1.6187 - val_accuracy: 0.4604 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.1202 - accuracy: 0.5792\n",
            "Epoch 29: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1202 - accuracy: 0.5792 - val_loss: 1.6095 - val_accuracy: 0.4598 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.1272 - accuracy: 0.5757\n",
            "Epoch 30: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 17ms/step - loss: 1.1272 - accuracy: 0.5757 - val_loss: 1.6218 - val_accuracy: 0.4620 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.1240 - accuracy: 0.5749\n",
            "Epoch 31: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1240 - accuracy: 0.5749 - val_loss: 1.6193 - val_accuracy: 0.4590 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.1229 - accuracy: 0.5767\n",
            "Epoch 32: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1229 - accuracy: 0.5766 - val_loss: 1.6291 - val_accuracy: 0.4588 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1286 - accuracy: 0.5773\n",
            "Epoch 33: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1287 - accuracy: 0.5771 - val_loss: 1.6202 - val_accuracy: 0.4592 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1324 - accuracy: 0.5726\n",
            "Epoch 34: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1325 - accuracy: 0.5727 - val_loss: 1.6323 - val_accuracy: 0.4592 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.1263 - accuracy: 0.5750\n",
            "Epoch 35: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1263 - accuracy: 0.5750 - val_loss: 1.6228 - val_accuracy: 0.4584 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.1252 - accuracy: 0.5774\n",
            "Epoch 36: val_loss did not improve from 1.50162\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1253 - accuracy: 0.5774 - val_loss: 1.6185 - val_accuracy: 0.4620 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1249 - accuracy: 0.5787\n",
            "Epoch 37: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1252 - accuracy: 0.5787 - val_loss: 1.6271 - val_accuracy: 0.4584 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1282 - accuracy: 0.5738\n",
            "Epoch 38: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1283 - accuracy: 0.5737 - val_loss: 1.6349 - val_accuracy: 0.4600 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1270 - accuracy: 0.5769\n",
            "Epoch 39: val_loss did not improve from 1.50162\n",
            "898/898 [==============================] - 16s 17ms/step - loss: 1.1273 - accuracy: 0.5769 - val_loss: 1.6195 - val_accuracy: 0.4574 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1211 - accuracy: 0.5800\n",
            "Epoch 40: val_loss did not improve from 1.50162\n",
            "Restoring model weights from the end of the best epoch: 30.\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.1209 - accuracy: 0.5801 - val_loss: 1.6255 - val_accuracy: 0.4572 - lr: 1.0000e-05\n",
            "Epoch 40: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8tv4YHkAUrh",
        "outputId": "f435d755-fd7f-475e-bc8d-a3a3d8551a1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 2s 19ms/step - loss: 1.6355 - accuracy: 0.4661\n",
            "Restored model, accuracy: 46.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "- Why is validation accuracy greater than training accuracy after adding dropout? \n",
        "- Why does my val loss doesn't decrease but val accuracy increases?\n",
        "\n",
        "## Notes\n",
        "\n",
        "**Momentum**\n",
        "\n",
        "momentum: that accelerates gradient descent in the relevant direction and dampens oscillations Momentum speeds up movement along directions of strong improvement (loss decrease) and also helps the network avoid local minima.\n",
        "\n",
        "**Nesterov momentum**\n",
        "\n",
        "nesterov momentum is a simple change to normal momentum. Here the gradient term is not computed from the current position\n",
        "while gradient may point in right direction, momentum may not always, so the intermediate position is found to redirect momentum \n",
        "\n",
        "## References \n",
        "\n",
        "1. https://dominikschmidt.xyz/nesterov-momentum/"
      ],
      "metadata": {
        "id": "EooDMtsmBvNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sFQSLEaYCA20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}