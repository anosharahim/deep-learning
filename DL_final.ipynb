{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_final",
      "provenance": [],
      "mount_file_id": "https://github.com/anosharahim/deep-learning/blob/master/DL_final.ipynb",
      "authorship_tag": "ABX9TyPiXhtq0RscXda/8c6ysakD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anosharahim/deep-learning/blob/master/DL_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and Libraries"
      ],
      "metadata": {
        "id": "zrSaV5p2_3t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import math \n",
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import batch_normalization\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau "
      ],
      "metadata": {
        "id": "G6xdo2BhxCnN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "oiq9fqNQ_2Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data from gdrive\n",
        "train_npz = np.load(\"/content/drive/MyDrive/Capstone/datasets/train.npz\")\n",
        "test_npz = np.load(\"/content/drive/MyDrive/Capstone/datasets/test.npz\")\n",
        "\n",
        "x_train = train_npz[\"arr_0\"]\n",
        "y_train = train_npz[\"arr_1\"]\n",
        "x_test = test_npz[\"arr_0\"]\n",
        "y_test = test_npz[\"arr_1\"]\n",
        "\n",
        "#add 2 more channels to grayscale image to imitate rgb\n",
        "x_train = np.repeat(x_train[..., np.newaxis], 3, -1)\n",
        "x_test = np.repeat(x_test[..., np.newaxis], 3, -1)\n",
        "\n",
        "#split test set into validation and test set \n",
        "x_val, x_test, y_val ,y_test = train_test_split(x_test,y_test, test_size=0.3)\n",
        "\n",
        "print('Train -- ', x_train.shape, y_train.shape)\n",
        "print('Val -- ', x_val.shape, y_val.shape)\n",
        "print('Test -- ', x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "UzbFTyF9xTcb",
        "outputId": "ea9fbf2f-90d7-483d-bc70-8f1ce249add9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train --  (28709, 48, 48, 3) (28709,)\n",
            "Val --  (5024, 48, 48, 3) (5024,)\n",
            "Test --  (2154, 48, 48, 3) (2154,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Model "
      ],
      "metadata": {
        "id": "MTGxYpJ2_7Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = VGG16(weights = 'imagenet', include_top=False, input_shape= (48,48,3))\n",
        "\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable=False\n",
        "\n",
        "for (i,layer) in enumerate(vgg.layers):\n",
        "  print(str(i)+' '+ layer.__class__.__name__, layer.trainable)\n",
        "\n",
        "def fully_connected(base_model, num_classes):\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dense(1024, activation='relu')(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dense(num_classes, activation='softmax')(x)\n",
        "  return x\n",
        "\n",
        "num_class = 7\n",
        "FC_head = fully_connected(vgg, num_class)\n",
        "model = Model(inputs=vgg.input, outputs = FC_head)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "lWjmwm1n8970",
        "outputId": "ef5d1e6b-65a9-4950-bed6-8eff185dcd2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 48, 48, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 48, 48, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 512)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1024)              1049600   \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,867,591\n",
            "Trainable params: 3,152,903\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training \n",
        "\n",
        "Serious overfitting rn"
      ],
      "metadata": {
        "id": "63932i95_-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "bs = 32\n",
        "rms_lr = 0.001\n",
        "sgd_lr = 0.01\n",
        "sgd_decay = 0.0001\n",
        "\n",
        "lrd = ReduceLROnPlateau(monitor='val_accuracy',mode='max',factor=0.3, patience=10, min_lr=0.00001, verbose=1)\n",
        "es = EarlyStopping(monitor = 'val_accuracy',min_delta = 0, patience = 10, verbose = 1, restore_best_weights = True)\n",
        "checkpoint = ModelCheckpoint('face_vgg.h5', monitor = 'val_loss', mode ='min', save_best_only = True, verbose=1)\n",
        "callbacks = [checkpoint, es, lrd]\n",
        "\n",
        "sgd = SGD(lr=sgd_lr, momentum=0.9, decay=sgd_decay, nesterov=True) \n",
        "rms = RMSprop(lr=rms_lr)\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, \n",
        "                    batch_size=bs, \n",
        "                    epochs=epochs, \n",
        "                    verbose=1,\n",
        "                    validation_data= (x_val, y_val),\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "n-fFpOXa8_zB",
        "outputId": "7eefcd6d-29f8-40eb-d00e-aaf1e143bb1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 2.0093 - accuracy: 0.3382\n",
            "Epoch 1: val_loss improved from inf to 1.62910, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 2.0092 - accuracy: 0.3382 - val_loss: 1.6291 - val_accuracy: 0.3770 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.6003 - accuracy: 0.3766\n",
            "Epoch 2: val_loss improved from 1.62910 to 1.60984, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 18ms/step - loss: 1.6003 - accuracy: 0.3766 - val_loss: 1.6098 - val_accuracy: 0.3812 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.5636 - accuracy: 0.3919\n",
            "Epoch 3: val_loss improved from 1.60984 to 1.58345, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 17ms/step - loss: 1.5637 - accuracy: 0.3919 - val_loss: 1.5835 - val_accuracy: 0.3796 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.5370 - accuracy: 0.4034\n",
            "Epoch 4: val_loss did not improve from 1.58345\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.5370 - accuracy: 0.4034 - val_loss: 1.5870 - val_accuracy: 0.3919 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.5081 - accuracy: 0.4144\n",
            "Epoch 5: val_loss improved from 1.58345 to 1.58135, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.5078 - accuracy: 0.4146 - val_loss: 1.5814 - val_accuracy: 0.4090 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.4739 - accuracy: 0.4292\n",
            "Epoch 6: val_loss improved from 1.58135 to 1.56996, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 16s 17ms/step - loss: 1.4741 - accuracy: 0.4291 - val_loss: 1.5700 - val_accuracy: 0.4025 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.4437 - accuracy: 0.4427\n",
            "Epoch 7: val_loss improved from 1.56996 to 1.54842, saving model to face_vgg.h5\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.4437 - accuracy: 0.4427 - val_loss: 1.5484 - val_accuracy: 0.4102 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.4070 - accuracy: 0.4588\n",
            "Epoch 8: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 1.4069 - accuracy: 0.4588 - val_loss: 1.5636 - val_accuracy: 0.4092 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.3651 - accuracy: 0.4774\n",
            "Epoch 9: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.3651 - accuracy: 0.4774 - val_loss: 1.5853 - val_accuracy: 0.4134 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.3257 - accuracy: 0.4949\n",
            "Epoch 10: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.3256 - accuracy: 0.4949 - val_loss: 1.6055 - val_accuracy: 0.4158 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 1.2858 - accuracy: 0.5093\n",
            "Epoch 11: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 1.2859 - accuracy: 0.5093 - val_loss: 1.6733 - val_accuracy: 0.4152 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.2421 - accuracy: 0.5265\n",
            "Epoch 12: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.2426 - accuracy: 0.5264 - val_loss: 1.6481 - val_accuracy: 0.4307 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "898/898 [==============================] - ETA: 0s - loss: 1.2060 - accuracy: 0.5450\n",
            "Epoch 13: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 1.2060 - accuracy: 0.5450 - val_loss: 1.6468 - val_accuracy: 0.4140 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 1.1582 - accuracy: 0.5646\n",
            "Epoch 14: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1582 - accuracy: 0.5647 - val_loss: 1.7103 - val_accuracy: 0.4291 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.1035 - accuracy: 0.5833\n",
            "Epoch 15: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.1035 - accuracy: 0.5833 - val_loss: 1.8129 - val_accuracy: 0.4357 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.0537 - accuracy: 0.6029\n",
            "Epoch 16: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 1.0537 - accuracy: 0.6029 - val_loss: 1.7965 - val_accuracy: 0.4271 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 1.0078 - accuracy: 0.6215\n",
            "Epoch 17: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 1.0078 - accuracy: 0.6214 - val_loss: 1.8760 - val_accuracy: 0.4285 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 0.9653 - accuracy: 0.6388\n",
            "Epoch 18: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.9656 - accuracy: 0.6387 - val_loss: 1.9483 - val_accuracy: 0.4244 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 0.9120 - accuracy: 0.6575\n",
            "Epoch 19: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.9124 - accuracy: 0.6572 - val_loss: 1.9901 - val_accuracy: 0.4258 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 0.8765 - accuracy: 0.6718\n",
            "Epoch 20: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.8767 - accuracy: 0.6718 - val_loss: 2.0661 - val_accuracy: 0.4441 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 0.8097 - accuracy: 0.6971\n",
            "Epoch 21: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.8095 - accuracy: 0.6971 - val_loss: 2.2079 - val_accuracy: 0.4331 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.7825 - accuracy: 0.7115\n",
            "Epoch 22: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.7826 - accuracy: 0.7115 - val_loss: 2.2226 - val_accuracy: 0.4477 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.7117 - accuracy: 0.7337\n",
            "Epoch 23: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.7117 - accuracy: 0.7337 - val_loss: 2.3863 - val_accuracy: 0.4461 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.6542 - accuracy: 0.7580\n",
            "Epoch 24: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.6542 - accuracy: 0.7580 - val_loss: 2.4554 - val_accuracy: 0.4494 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 0.6378 - accuracy: 0.7667\n",
            "Epoch 25: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 0.6379 - accuracy: 0.7666 - val_loss: 2.5798 - val_accuracy: 0.4268 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.5958 - accuracy: 0.7834\n",
            "Epoch 26: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.5958 - accuracy: 0.7833 - val_loss: 2.5935 - val_accuracy: 0.4393 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 0.5302 - accuracy: 0.8088\n",
            "Epoch 27: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 0.5302 - accuracy: 0.8088 - val_loss: 2.8423 - val_accuracy: 0.4329 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.8137\n",
            "Epoch 28: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.5146 - accuracy: 0.8136 - val_loss: 2.9056 - val_accuracy: 0.4309 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.8370\n",
            "Epoch 29: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.4540 - accuracy: 0.8370 - val_loss: 3.0608 - val_accuracy: 0.4375 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8495\n",
            "Epoch 30: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.4197 - accuracy: 0.8495 - val_loss: 3.1112 - val_accuracy: 0.4319 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 0.3844 - accuracy: 0.8624\n",
            "Epoch 31: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 0.3845 - accuracy: 0.8624 - val_loss: 3.2973 - val_accuracy: 0.4433 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "897/898 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.8813\n",
            "Epoch 32: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.3309 - accuracy: 0.8813 - val_loss: 3.5639 - val_accuracy: 0.4490 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "895/898 [============================>.] - ETA: 0s - loss: 0.3033 - accuracy: 0.8948\n",
            "Epoch 33: val_loss did not improve from 1.54842\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.3034 - accuracy: 0.8947 - val_loss: 3.6703 - val_accuracy: 0.4425 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "896/898 [============================>.] - ETA: 0s - loss: 0.2566 - accuracy: 0.9130\n",
            "Epoch 34: val_loss did not improve from 1.54842\n",
            "Restoring model weights from the end of the best epoch: 24.\n",
            "\n",
            "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
            "898/898 [==============================] - 15s 17ms/step - loss: 0.2565 - accuracy: 0.9131 - val_loss: 3.8145 - val_accuracy: 0.4463 - lr: 0.0100\n",
            "Epoch 34: early stopping\n"
          ]
        }
      ]
    }
  ]
}