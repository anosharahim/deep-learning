{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c474a39c",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "In this project, I have built a fully-connected neural network from scratch using only Numpy to perform digit classification on the MNIST dataset. \n",
    "\n",
    "\n",
    "\n",
    "## Requirements \n",
    "\n",
    "- A working fully-connected deep neural network from scratch using only numpy.\n",
    "- Includes dense layers, activations, optimizers, loss functions and sigmoid or softmax in case of classification. \n",
    "- Runtime and results on a public dataset.\n",
    "- Documented code that includes brief summary, technical details, and results. \n",
    "\n",
    "## Extensions \n",
    "\n",
    "A comparison of the modelâ€™s runtime and performance with/without each component:\n",
    "- More than 1 optimizer - SGD, Momentum, RMSProp, Adam etc.\n",
    "- Regularization - L2/weight decay, dropout, possibly augmentations if image data etc.\n",
    "- Results on more than 1 dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fab3b7",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "First, I load the MNIST dataset of 70,000 digit images from sklearn and split it into training (60,000 images) and test set (10,000 images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbbc4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## load MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "X = X /255 #normalize for simplicity \n",
    "\n",
    "#split into test and train sets \n",
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = np.array(X[:m]), np.array(X[m:])\n",
    "\n",
    "#one-hot encode mnist labels for classification \n",
    "digits =10 \n",
    "examples = 70000\n",
    "y_new = np.array(y, dtype='int32').reshape(-1)\n",
    "y_new = np.eye(10)[y_new]\n",
    "Y_train, Y_test = np.array(y_new[:m]), np.array(y_new[m:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa27ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions for neural network \n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def der_sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def cross_entropy(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Binary cross entropy loss function.\n",
    "    \"\"\"\n",
    "    L = -np.log(np.max(Y_hat * Y, axis=1) + 1e-8) #add small number to avoid zero error\n",
    "    L = np.average(L)\n",
    "    return L\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Computes softmax values for each sets of scores in x.\"\"\"\n",
    "    prob = []\n",
    "    for arr in x:\n",
    "        exponentiated = [np.exp(x) for x in arr]\n",
    "        denominator = np.sum(exponentiated)\n",
    "        prob.append([x / denominator for x in exponentiated])\n",
    "    return np.array(prob)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    '''Computes accuracy of model preDictions .\n",
    "    '''\n",
    "    bools = np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)\n",
    "    return np.average(bools.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c8fb26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 #the input layer has 784 pixels as nodes \n",
    "learning_rate = 5 \n",
    "batch_size = 60 #specify batch size for mini-batch gradient descent \n",
    "\n",
    "#Initialization: weights and biases\n",
    "\n",
    "#weights can be very small non-zero random numbers \n",
    "w1 = 0.001 * np.random.randn(input_size,64) \n",
    "w2 = 0.001 * np.random.randn(64,10)\n",
    "#biases can be zeros at the start \n",
    "b1 = np.zeros(64) \n",
    "b2 = np.zeros(10)\n",
    "\n",
    "def gradient_descent(X, y, forward_only=False):\n",
    "    \"\"\"Performs gradient descent on MNIST data to compute loss and accuracy. \n",
    "    \"\"\"\n",
    "    \n",
    "    #weights and biases are globally initialized \n",
    "    global w2\n",
    "    global b2\n",
    "    global w1\n",
    "    global b1\n",
    "    \n",
    "    #### Forward Propagation ####\n",
    "    \n",
    "    z1 = np.dot(X,w1) + b1 #multipy weights by input nodes and add bias \n",
    "    s1 = sigmoid(z1) #put it through the activation function \n",
    "    z2 = np.dot(s1,w2) + b2  #multipy weights by previous nodes and add bias \n",
    "    y_hat = softmax(z2) #softmax outputs probabilities as the model output \n",
    "    L = cross_entropy(y,y_hat) #compute loss using model output and true labels\n",
    "    acc = accuracy(y, y_hat) #compute accuracy of predictions\n",
    "    \n",
    "    if forward_only: #for testing, only forward propagate  \n",
    "        return L, acc\n",
    "    \n",
    "    \n",
    "    #### Backward propagation ####\n",
    "    \n",
    "    #Apply chain rule on each layer to see how cost changes with respect to \n",
    "    #each component of weights matrix and each bias \n",
    "    \n",
    "    #Gradients for Layer 2 \n",
    "    d_z2 = y_hat - y \n",
    "    d_w2 = s1.T.dot(d_z2) # \n",
    "    d_b2 = np.sum(d_z2, axis=0)\n",
    "    d_s1 = der_sigmoid(z1) * d_z2.dot(w2.T)\n",
    "    \n",
    "    #Gradients for Layer 1\n",
    "    d_w1 = X.T.dot(d_s1)\n",
    "    d_b1 = np.sum(d_w1, axis=0)\n",
    "    \n",
    "    #Use gradients to update weights and biases\n",
    "    w2 = (w2 - (learning_rate * (d_w2 / m)))\n",
    "    b2 = (b2 - (learning_rate * (d_b2 / m)))\n",
    "    w1 = (w1 - (learning_rate * (d_w1 / m)))\n",
    "    b1 = (b1 - (learning_rate * (d_b1 / m)))\n",
    "    \n",
    "    return L, acc\n",
    "\n",
    "\n",
    "def run_epoch(X, y, n, test=False):\n",
    "    '''Computes loss and accuracy for one epoch using mini-batch gradient descent. \n",
    "    '''\n",
    "    index = np.arange(n)\n",
    "    np.random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    batch_count = m / batch_size\n",
    "    X_batches = np.array_split(X, batch_count)\n",
    "    y_batches = np.array_split(y, batch_count)\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    for i in range(len(X_batches)):\n",
    "        loss, acc = gradient_descent(X_batches[i], y_batches[i], forward_only=test)\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "    return np.average(losses), np.average(accs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5869d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.301538099126011, Accuracy: 0.11166666666666668\n",
      "Epoch 1 Loss: 2.3012718229577405, Accuracy: 0.11236666666666667\n",
      "Epoch 2 Loss: 2.300794783400451, Accuracy: 0.11258333333333333\n",
      "Epoch 3 Loss: 2.2996924550215083, Accuracy: 0.1124\n",
      "Epoch 4 Loss: 2.2958894411352184, Accuracy: 0.11236666666666667\n",
      "Epoch 5 Loss: 2.284504548654145, Accuracy: 0.13163333333333332\n",
      "Epoch 6 Loss: 2.255730325322106, Accuracy: 0.22838333333333333\n",
      "Epoch 7 Loss: 2.189712702123951, Accuracy: 0.2896166666666667\n",
      "Epoch 8 Loss: 2.0540259256795053, Accuracy: 0.3589833333333333\n",
      "Epoch 9 Loss: 1.8502293787023045, Accuracy: 0.4413833333333334\n",
      "Epoch 10 Loss: 1.6391696611763573, Accuracy: 0.5096666666666667\n",
      "Epoch 11 Loss: 1.4574235042145696, Accuracy: 0.5612166666666667\n",
      "Epoch 12 Loss: 1.3090576052046663, Accuracy: 0.6020666666666666\n",
      "Epoch 13 Loss: 1.1905702093264439, Accuracy: 0.6428166666666666\n",
      "Epoch 14 Loss: 1.0954028320980067, Accuracy: 0.6780166666666666\n",
      "Epoch 15 Loss: 1.0167923297421206, Accuracy: 0.7087666666666667\n",
      "Epoch 16 Loss: 0.9490643524981169, Accuracy: 0.7351500000000001\n",
      "Epoch 17 Loss: 0.8894182090618767, Accuracy: 0.75585\n",
      "Epoch 18 Loss: 0.8363802506061856, Accuracy: 0.7754333333333334\n",
      "Epoch 19 Loss: 0.7892996615403137, Accuracy: 0.7894\n",
      "Epoch 20 Loss: 0.7477072241720426, Accuracy: 0.8022666666666668\n",
      "Epoch 21 Loss: 0.7111836049045065, Accuracy: 0.8122166666666667\n",
      "Epoch 22 Loss: 0.6786266501392992, Accuracy: 0.8206166666666665\n",
      "Epoch 23 Loss: 0.6504285134715567, Accuracy: 0.8283499999999999\n",
      "Epoch 24 Loss: 0.6252611883176119, Accuracy: 0.8349333333333334\n",
      "Epoch 25 Loss: 0.6029309724103109, Accuracy: 0.84125\n",
      "Epoch 26 Loss: 0.582842914965237, Accuracy: 0.8463166666666666\n",
      "Epoch 27 Loss: 0.5648389891621726, Accuracy: 0.8511166666666665\n",
      "Epoch 28 Loss: 0.5485459277102737, Accuracy: 0.8561333333333334\n",
      "Epoch 29 Loss: 0.5335839821004623, Accuracy: 0.8598000000000001\n",
      "Epoch 30 Loss: 0.5197704267124663, Accuracy: 0.8634666666666667\n",
      "Epoch 31 Loss: 0.5072460327418443, Accuracy: 0.8669166666666667\n",
      "Epoch 32 Loss: 0.49546031211769864, Accuracy: 0.86975\n",
      "Epoch 33 Loss: 0.4847115708770473, Accuracy: 0.8725333333333333\n",
      "Epoch 34 Loss: 0.47467058819428826, Accuracy: 0.8748166666666666\n",
      "Epoch 35 Loss: 0.4652947010273493, Accuracy: 0.8769833333333333\n",
      "Epoch 36 Loss: 0.45648383485720034, Accuracy: 0.8792\n",
      "Epoch 37 Loss: 0.44827261571430277, Accuracy: 0.8808833333333334\n",
      "Epoch 38 Loss: 0.4406649285969773, Accuracy: 0.8832833333333333\n",
      "Epoch 39 Loss: 0.4334233414144227, Accuracy: 0.8847333333333334\n",
      "Epoch 40 Loss: 0.4267549427252243, Accuracy: 0.8860999999999999\n",
      "Epoch 41 Loss: 0.4202769476777331, Accuracy: 0.8874333333333334\n",
      "Epoch 42 Loss: 0.4144526658176637, Accuracy: 0.8890833333333332\n",
      "Epoch 43 Loss: 0.4087229692869472, Accuracy: 0.8902166666666667\n",
      "Epoch 44 Loss: 0.4034665465867805, Accuracy: 0.8915\n",
      "Epoch 45 Loss: 0.398358974027931, Accuracy: 0.893\n",
      "Epoch 46 Loss: 0.39355961178492294, Accuracy: 0.8937166666666667\n",
      "Epoch 47 Loss: 0.38909278899538263, Accuracy: 0.8952166666666667\n",
      "Epoch 48 Loss: 0.38471483985418, Accuracy: 0.8957833333333333\n",
      "Epoch 49 Loss: 0.3807704691375897, Accuracy: 0.8966666666666666\n",
      "Epoch 50 Loss: 0.37680024997942446, Accuracy: 0.8977166666666667\n",
      "Epoch 51 Loss: 0.37315973356403515, Accuracy: 0.8987666666666666\n",
      "Epoch 52 Loss: 0.3694441741181839, Accuracy: 0.8997999999999999\n",
      "Epoch 53 Loss: 0.36602071000302205, Accuracy: 0.9005666666666666\n",
      "Epoch 54 Loss: 0.3628764659521874, Accuracy: 0.9011666666666667\n",
      "Epoch 55 Loss: 0.35964523623551675, Accuracy: 0.9023666666666665\n",
      "Epoch 56 Loss: 0.3566566156919961, Accuracy: 0.9027833333333333\n",
      "Epoch 57 Loss: 0.3536925615084368, Accuracy: 0.9037333333333334\n",
      "Epoch 58 Loss: 0.3509032036978897, Accuracy: 0.9044500000000001\n",
      "Epoch 59 Loss: 0.3482984121975316, Accuracy: 0.905\n",
      "Epoch 60 Loss: 0.3455011223168609, Accuracy: 0.9055666666666666\n",
      "Epoch 61 Loss: 0.3430898228144644, Accuracy: 0.9061999999999999\n",
      "Epoch 62 Loss: 0.3405645586015263, Accuracy: 0.9064999999999999\n",
      "Epoch 63 Loss: 0.3382134240696407, Accuracy: 0.9073333333333332\n",
      "Epoch 64 Loss: 0.33576337760417196, Accuracy: 0.9077000000000001\n",
      "Epoch 65 Loss: 0.33352835360455707, Accuracy: 0.9085833333333333\n",
      "Epoch 66 Loss: 0.3313323087457128, Accuracy: 0.9091333333333333\n",
      "Epoch 67 Loss: 0.3291794244530054, Accuracy: 0.9096333333333333\n",
      "Epoch 68 Loss: 0.3272258409951082, Accuracy: 0.9097666666666666\n",
      "Epoch 69 Loss: 0.3250880758203032, Accuracy: 0.9103166666666666\n",
      "Epoch 70 Loss: 0.3230156686465846, Accuracy: 0.9115166666666666\n",
      "Epoch 71 Loss: 0.32113475924346685, Accuracy: 0.9114666666666666\n",
      "Epoch 72 Loss: 0.3192834885425651, Accuracy: 0.9119333333333332\n",
      "Epoch 73 Loss: 0.317277565592027, Accuracy: 0.9126333333333332\n",
      "Epoch 74 Loss: 0.3154424020348125, Accuracy: 0.9132499999999999\n",
      "Epoch 75 Loss: 0.3136825656158478, Accuracy: 0.9136166666666665\n",
      "Epoch 76 Loss: 0.3119755774202528, Accuracy: 0.9142\n",
      "Epoch 77 Loss: 0.31020781795002567, Accuracy: 0.9145666666666666\n",
      "Epoch 78 Loss: 0.30857676154878866, Accuracy: 0.9148\n",
      "Epoch 79 Loss: 0.3068635943491917, Accuracy: 0.9152166666666666\n",
      "Epoch 80 Loss: 0.3052710415880568, Accuracy: 0.9157833333333333\n",
      "Epoch 81 Loss: 0.3035912791120493, Accuracy: 0.9163833333333332\n",
      "Epoch 82 Loss: 0.30205944775414084, Accuracy: 0.9165\n",
      "Epoch 83 Loss: 0.30056772471541626, Accuracy: 0.9172333333333333\n",
      "Epoch 84 Loss: 0.2990330843852247, Accuracy: 0.9174999999999999\n",
      "Epoch 85 Loss: 0.2974874524270749, Accuracy: 0.9180833333333333\n",
      "Epoch 86 Loss: 0.2960907178923368, Accuracy: 0.9185333333333333\n",
      "Epoch 87 Loss: 0.2945510931559733, Accuracy: 0.9188333333333334\n",
      "Epoch 88 Loss: 0.2930842320125216, Accuracy: 0.9194\n",
      "Epoch 89 Loss: 0.2917634904610085, Accuracy: 0.9196666666666666\n",
      "Epoch 90 Loss: 0.29042499513087805, Accuracy: 0.9203166666666667\n",
      "Epoch 91 Loss: 0.28894453579898643, Accuracy: 0.92045\n",
      "Epoch 92 Loss: 0.28773287580356083, Accuracy: 0.9208666666666666\n",
      "Epoch 93 Loss: 0.2864233428644964, Accuracy: 0.9213166666666666\n",
      "Epoch 94 Loss: 0.2851736980459544, Accuracy: 0.92165\n",
      "Epoch 95 Loss: 0.28374930132717263, Accuracy: 0.9218666666666667\n",
      "Epoch 96 Loss: 0.2824651383768868, Accuracy: 0.922\n",
      "Epoch 97 Loss: 0.2812292297138926, Accuracy: 0.9223333333333332\n",
      "Epoch 98 Loss: 0.2800546817289889, Accuracy: 0.9228999999999999\n",
      "Epoch 99 Loss: 0.27877607288974204, Accuracy: 0.9233166666666666\n",
      "Epoch 100 Loss: 0.2775736469952334, Accuracy: 0.9232166666666666\n",
      "Epoch 101 Loss: 0.27641673606765654, Accuracy: 0.9237333333333332\n",
      "Epoch 102 Loss: 0.27522288671957595, Accuracy: 0.9242666666666667\n",
      "Epoch 103 Loss: 0.27414995891461785, Accuracy: 0.9245666666666666\n",
      "Epoch 104 Loss: 0.2728815074499268, Accuracy: 0.9245166666666667\n",
      "Epoch 105 Loss: 0.2718333841804563, Accuracy: 0.9250999999999999\n",
      "Epoch 106 Loss: 0.2706935782682724, Accuracy: 0.9253333333333332\n",
      "Epoch 107 Loss: 0.2695955200526998, Accuracy: 0.9259166666666666\n",
      "Epoch 108 Loss: 0.2685328213528319, Accuracy: 0.9257166666666667\n",
      "Epoch 109 Loss: 0.2673649080143292, Accuracy: 0.9260999999999999\n",
      "Epoch 110 Loss: 0.2663660861812318, Accuracy: 0.9261999999999999\n",
      "Epoch 111 Loss: 0.26531550173054186, Accuracy: 0.9266166666666666\n",
      "Epoch 112 Loss: 0.26426132755235343, Accuracy: 0.92685\n",
      "Epoch 113 Loss: 0.2632456041425537, Accuracy: 0.927\n",
      "Epoch 114 Loss: 0.2622383914007905, Accuracy: 0.9273166666666666\n",
      "Epoch 115 Loss: 0.26114645488048266, Accuracy: 0.9273499999999999\n",
      "Epoch 116 Loss: 0.2602161677149498, Accuracy: 0.9277499999999999\n",
      "Epoch 117 Loss: 0.2592078105876807, Accuracy: 0.9281166666666666\n",
      "Epoch 118 Loss: 0.25818069761853996, Accuracy: 0.9284\n",
      "Epoch 119 Loss: 0.2572813107880126, Accuracy: 0.9287499999999999\n",
      "Epoch 120 Loss: 0.2563191175244477, Accuracy: 0.9289999999999999\n",
      "Epoch 121 Loss: 0.2552871480371137, Accuracy: 0.9290499999999999\n",
      "Epoch 122 Loss: 0.25440659501383933, Accuracy: 0.9293666666666666\n",
      "Epoch 123 Loss: 0.253431973011833, Accuracy: 0.9295999999999999\n",
      "Epoch 124 Loss: 0.25259425460838547, Accuracy: 0.9299666666666666\n",
      "Epoch 125 Loss: 0.2515029608934225, Accuracy: 0.9298833333333332\n",
      "Epoch 126 Loss: 0.25067635983437037, Accuracy: 0.9301166666666666\n",
      "Epoch 127 Loss: 0.2497728535294903, Accuracy: 0.9304333333333334\n",
      "Epoch 128 Loss: 0.24888437374087588, Accuracy: 0.9307333333333333\n",
      "Epoch 129 Loss: 0.24797177055227138, Accuracy: 0.9307166666666667\n",
      "Epoch 130 Loss: 0.2470961542342633, Accuracy: 0.9312666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131 Loss: 0.2461755100601381, Accuracy: 0.9315999999999999\n",
      "Epoch 132 Loss: 0.2454709770617752, Accuracy: 0.9316833333333333\n",
      "Epoch 133 Loss: 0.24442040577854102, Accuracy: 0.9318166666666666\n",
      "Epoch 134 Loss: 0.24357995652488607, Accuracy: 0.9321333333333334\n",
      "Epoch 135 Loss: 0.24287777592064083, Accuracy: 0.9323333333333332\n",
      "Epoch 136 Loss: 0.24193186035609143, Accuracy: 0.9327\n",
      "Epoch 137 Loss: 0.2411175498440461, Accuracy: 0.9323666666666666\n",
      "Epoch 138 Loss: 0.24030305826323967, Accuracy: 0.9328333333333333\n",
      "Epoch 139 Loss: 0.23952270724787555, Accuracy: 0.9330833333333333\n",
      "Epoch 140 Loss: 0.2385802672196537, Accuracy: 0.9333499999999999\n",
      "Epoch 141 Loss: 0.23780659134934096, Accuracy: 0.9337499999999999\n",
      "Epoch 142 Loss: 0.23704795326002293, Accuracy: 0.9337666666666666\n",
      "Epoch 143 Loss: 0.23625913682412017, Accuracy: 0.9339666666666666\n",
      "Epoch 144 Loss: 0.2354047613089837, Accuracy: 0.9341166666666666\n",
      "Epoch 145 Loss: 0.23474852875474805, Accuracy: 0.9344333333333332\n",
      "Epoch 146 Loss: 0.2338990321789359, Accuracy: 0.93455\n",
      "Epoch 147 Loss: 0.23312459752468745, Accuracy: 0.9348666666666665\n",
      "Epoch 148 Loss: 0.2323156195772857, Accuracy: 0.9350666666666666\n",
      "Epoch 149 Loss: 0.23156556135793582, Accuracy: 0.9354999999999999\n",
      "Epoch 150 Loss: 0.23076880374875205, Accuracy: 0.9353833333333332\n",
      "Epoch 151 Loss: 0.23002037036678702, Accuracy: 0.9356833333333333\n",
      "Epoch 152 Loss: 0.22931300841923155, Accuracy: 0.9356499999999999\n",
      "Epoch 153 Loss: 0.2286455168553871, Accuracy: 0.9359333333333333\n",
      "Epoch 154 Loss: 0.22786727716185004, Accuracy: 0.9362833333333334\n",
      "Epoch 155 Loss: 0.22703562840837946, Accuracy: 0.93655\n",
      "Epoch 156 Loss: 0.2263567153584856, Accuracy: 0.9367499999999999\n",
      "Epoch 157 Loss: 0.22569263031800824, Accuracy: 0.9370666666666666\n",
      "Epoch 158 Loss: 0.22496315758653104, Accuracy: 0.9370833333333333\n",
      "Epoch 159 Loss: 0.22417977393818495, Accuracy: 0.9374666666666666\n",
      "Epoch 160 Loss: 0.22353795582711006, Accuracy: 0.9377166666666666\n",
      "Epoch 161 Loss: 0.22281132517020272, Accuracy: 0.9378999999999998\n",
      "Epoch 162 Loss: 0.22207145800853015, Accuracy: 0.9380999999999999\n",
      "Epoch 163 Loss: 0.22148541535589897, Accuracy: 0.9382833333333334\n",
      "Epoch 164 Loss: 0.220680149789625, Accuracy: 0.9383333333333332\n",
      "Epoch 165 Loss: 0.22003072828865844, Accuracy: 0.9385999999999999\n",
      "Epoch 166 Loss: 0.21940089490084874, Accuracy: 0.9386833333333332\n",
      "Epoch 167 Loss: 0.21874958479726098, Accuracy: 0.9393499999999999\n",
      "Epoch 168 Loss: 0.21804228094993194, Accuracy: 0.9395166666666667\n",
      "Epoch 169 Loss: 0.21736186192437368, Accuracy: 0.9394\n",
      "Epoch 170 Loss: 0.21677895045748816, Accuracy: 0.9398\n",
      "Epoch 171 Loss: 0.2160693904432653, Accuracy: 0.9400666666666666\n",
      "Epoch 172 Loss: 0.21537784854758413, Accuracy: 0.9402833333333332\n",
      "Epoch 173 Loss: 0.21473383593990253, Accuracy: 0.9402333333333331\n",
      "Epoch 174 Loss: 0.21411664054921103, Accuracy: 0.9407666666666665\n",
      "Epoch 175 Loss: 0.21342980813104553, Accuracy: 0.9407833333333332\n",
      "Epoch 176 Loss: 0.2128546354370787, Accuracy: 0.9410166666666666\n",
      "Epoch 177 Loss: 0.21208880132110114, Accuracy: 0.9412833333333332\n",
      "Epoch 178 Loss: 0.2116464348619091, Accuracy: 0.9410166666666666\n",
      "Epoch 179 Loss: 0.2109479359360385, Accuracy: 0.94155\n",
      "Epoch 180 Loss: 0.21040674037356258, Accuracy: 0.9413333333333332\n",
      "Epoch 181 Loss: 0.20968440289698367, Accuracy: 0.94175\n",
      "Epoch 182 Loss: 0.2090624371611481, Accuracy: 0.9419833333333333\n",
      "Epoch 183 Loss: 0.20857377846034336, Accuracy: 0.9420666666666666\n",
      "Epoch 184 Loss: 0.20793116641574502, Accuracy: 0.9423833333333332\n",
      "Epoch 185 Loss: 0.20730119753196316, Accuracy: 0.9424999999999999\n",
      "Epoch 186 Loss: 0.20680513377126847, Accuracy: 0.9424166666666666\n",
      "Epoch 187 Loss: 0.20609635123980868, Accuracy: 0.9428833333333333\n",
      "Epoch 188 Loss: 0.2055169161856447, Accuracy: 0.9431166666666666\n",
      "Epoch 189 Loss: 0.2050546263665477, Accuracy: 0.9429833333333334\n",
      "Epoch 190 Loss: 0.20435100251061944, Accuracy: 0.9431499999999998\n",
      "Epoch 191 Loss: 0.20384672343608878, Accuracy: 0.94365\n",
      "Epoch 192 Loss: 0.20329163171564615, Accuracy: 0.94325\n",
      "Epoch 193 Loss: 0.20270306491348034, Accuracy: 0.9438833333333332\n",
      "Epoch 194 Loss: 0.20207726630340836, Accuracy: 0.9437833333333333\n",
      "Epoch 195 Loss: 0.20161358926332082, Accuracy: 0.9439833333333333\n",
      "Epoch 196 Loss: 0.20105181829150578, Accuracy: 0.9444499999999999\n",
      "Epoch 197 Loss: 0.20051705569899364, Accuracy: 0.9445166666666666\n",
      "Epoch 198 Loss: 0.1998752621429828, Accuracy: 0.9446166666666665\n",
      "Epoch 199 Loss: 0.19938819727110685, Accuracy: 0.9449333333333333\n",
      "Epoch 200 Loss: 0.19887685076220132, Accuracy: 0.9446999999999999\n",
      "Epoch 201 Loss: 0.1982960387513064, Accuracy: 0.9453333333333332\n",
      "Epoch 202 Loss: 0.19780536167252757, Accuracy: 0.9452999999999999\n",
      "Epoch 203 Loss: 0.19725827598846257, Accuracy: 0.9452833333333333\n",
      "Epoch 204 Loss: 0.19675395725756248, Accuracy: 0.9454999999999999\n",
      "Epoch 205 Loss: 0.1962753204035743, Accuracy: 0.9459333333333332\n",
      "Epoch 206 Loss: 0.19562125416783138, Accuracy: 0.9460333333333333\n",
      "Epoch 207 Loss: 0.1951761029779458, Accuracy: 0.946\n",
      "Epoch 208 Loss: 0.1946816945244648, Accuracy: 0.9462999999999998\n",
      "Epoch 209 Loss: 0.19422480148387297, Accuracy: 0.9465333333333333\n",
      "Epoch 210 Loss: 0.193701240355584, Accuracy: 0.9467833333333333\n",
      "Epoch 211 Loss: 0.19308385085017857, Accuracy: 0.94695\n",
      "Epoch 212 Loss: 0.1927188554763753, Accuracy: 0.9468999999999999\n",
      "Epoch 213 Loss: 0.1921607124854116, Accuracy: 0.9472999999999999\n",
      "Epoch 214 Loss: 0.1916734556074758, Accuracy: 0.9473166666666666\n",
      "Epoch 215 Loss: 0.19119431696381667, Accuracy: 0.9474166666666666\n",
      "Epoch 216 Loss: 0.1907020955504218, Accuracy: 0.9475666666666666\n",
      "Epoch 217 Loss: 0.19028604575514613, Accuracy: 0.9477333333333332\n",
      "Epoch 218 Loss: 0.1897214310705551, Accuracy: 0.9478166666666666\n",
      "Epoch 219 Loss: 0.18927693608405421, Accuracy: 0.9480166666666665\n",
      "Epoch 220 Loss: 0.18880447762010782, Accuracy: 0.9479666666666666\n",
      "Epoch 221 Loss: 0.1883158827965658, Accuracy: 0.9479833333333331\n",
      "Epoch 222 Loss: 0.18782688810060633, Accuracy: 0.94855\n",
      "Epoch 223 Loss: 0.18737193856891482, Accuracy: 0.9483166666666666\n",
      "Epoch 224 Loss: 0.18698586022395491, Accuracy: 0.9485999999999999\n",
      "Epoch 225 Loss: 0.1864541313620654, Accuracy: 0.9485499999999999\n",
      "Epoch 226 Loss: 0.18601863674391644, Accuracy: 0.9486499999999999\n",
      "Epoch 227 Loss: 0.18565376931876518, Accuracy: 0.9488333333333333\n",
      "Epoch 228 Loss: 0.18513344267448334, Accuracy: 0.9490666666666666\n",
      "Epoch 229 Loss: 0.18460965560694478, Accuracy: 0.9491166666666666\n",
      "Epoch 230 Loss: 0.18421966685626684, Accuracy: 0.9491499999999998\n",
      "Epoch 231 Loss: 0.18384003264707094, Accuracy: 0.9490833333333333\n",
      "Epoch 232 Loss: 0.1833685913966635, Accuracy: 0.9492333333333333\n",
      "Epoch 233 Loss: 0.18297365145872546, Accuracy: 0.9494499999999998\n",
      "Epoch 234 Loss: 0.18243238717005572, Accuracy: 0.9497666666666666\n",
      "Epoch 235 Loss: 0.18203197756859632, Accuracy: 0.9497499999999999\n",
      "Epoch 236 Loss: 0.18151406303130463, Accuracy: 0.9499333333333333\n",
      "Epoch 237 Loss: 0.18119670289117557, Accuracy: 0.9497833333333331\n",
      "Epoch 238 Loss: 0.18071966987885008, Accuracy: 0.9500666666666666\n",
      "Epoch 239 Loss: 0.1803954340551573, Accuracy: 0.9502666666666666\n",
      "Epoch 240 Loss: 0.17988598967545277, Accuracy: 0.9501666666666665\n",
      "Epoch 241 Loss: 0.17946121103847254, Accuracy: 0.9505333333333333\n",
      "Epoch 242 Loss: 0.17910431104929303, Accuracy: 0.9503499999999999\n",
      "Epoch 243 Loss: 0.17869708090845415, Accuracy: 0.9505333333333332\n",
      "Epoch 244 Loss: 0.178271841051286, Accuracy: 0.9506833333333333\n",
      "Epoch 245 Loss: 0.17788449808367257, Accuracy: 0.9508166666666666\n",
      "Epoch 246 Loss: 0.17744989135786315, Accuracy: 0.9508\n",
      "Epoch 247 Loss: 0.17708351389628366, Accuracy: 0.9508999999999999\n",
      "Epoch 248 Loss: 0.17662311727531194, Accuracy: 0.95125\n",
      "Epoch 249 Loss: 0.17623896335282083, Accuracy: 0.9511999999999998\n",
      "Epoch 250 Loss: 0.17586719529821823, Accuracy: 0.9512999999999998\n",
      "Epoch 251 Loss: 0.17551727812245144, Accuracy: 0.9513333333333333\n",
      "Epoch 252 Loss: 0.17503785633115942, Accuracy: 0.9518166666666666\n",
      "Epoch 253 Loss: 0.17470225825118246, Accuracy: 0.9516666666666665\n",
      "Epoch 254 Loss: 0.17434714277533464, Accuracy: 0.9518499999999999\n",
      "Epoch 255 Loss: 0.1738644923316497, Accuracy: 0.9518333333333333\n",
      "Epoch 256 Loss: 0.1735840453157619, Accuracy: 0.9520666666666666\n",
      "Epoch 257 Loss: 0.17309516192267604, Accuracy: 0.9520833333333333\n",
      "Epoch 258 Loss: 0.17276332999606756, Accuracy: 0.9523666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259 Loss: 0.17246251915333569, Accuracy: 0.9525833333333332\n",
      "Epoch 260 Loss: 0.17199919739046315, Accuracy: 0.9526499999999999\n",
      "Epoch 261 Loss: 0.17165594042058574, Accuracy: 0.9525333333333333\n",
      "Epoch 262 Loss: 0.17131379813170436, Accuracy: 0.9525999999999999\n",
      "Epoch 263 Loss: 0.17092954499258098, Accuracy: 0.9527666666666665\n",
      "Epoch 264 Loss: 0.17054495621523483, Accuracy: 0.9529666666666666\n",
      "Epoch 265 Loss: 0.17021465016863419, Accuracy: 0.9529499999999999\n",
      "Epoch 266 Loss: 0.16981347953174236, Accuracy: 0.9532333333333333\n",
      "Epoch 267 Loss: 0.16945917157457485, Accuracy: 0.9531166666666666\n",
      "Epoch 268 Loss: 0.16905718665726657, Accuracy: 0.9533666666666666\n",
      "Epoch 269 Loss: 0.16874019708799207, Accuracy: 0.9533999999999999\n",
      "Epoch 270 Loss: 0.1683854711369862, Accuracy: 0.9535\n",
      "Epoch 271 Loss: 0.16798435920686103, Accuracy: 0.9536333333333332\n",
      "Epoch 272 Loss: 0.16765305390123256, Accuracy: 0.9538666666666665\n",
      "Epoch 273 Loss: 0.1673344218291963, Accuracy: 0.9538166666666666\n",
      "Epoch 274 Loss: 0.16700762623711166, Accuracy: 0.9539166666666666\n",
      "Epoch 275 Loss: 0.1666312758104493, Accuracy: 0.9540833333333333\n",
      "Epoch 276 Loss: 0.16629496629605794, Accuracy: 0.9538999999999999\n",
      "Epoch 277 Loss: 0.1659157282426678, Accuracy: 0.9541166666666666\n",
      "Epoch 278 Loss: 0.16560253753835563, Accuracy: 0.9541999999999999\n",
      "Epoch 279 Loss: 0.16526839011001027, Accuracy: 0.9544666666666665\n",
      "Epoch 280 Loss: 0.16490157151987136, Accuracy: 0.9542166666666666\n",
      "Epoch 281 Loss: 0.16457794728033157, Accuracy: 0.9544166666666666\n",
      "Epoch 282 Loss: 0.1642457887126114, Accuracy: 0.9547666666666665\n",
      "Epoch 283 Loss: 0.16392943995864567, Accuracy: 0.9548666666666665\n",
      "Epoch 284 Loss: 0.16364162819252084, Accuracy: 0.9548\n",
      "Epoch 285 Loss: 0.16330153379669027, Accuracy: 0.9548666666666665\n",
      "Epoch 286 Loss: 0.16295102667418843, Accuracy: 0.9548\n",
      "Epoch 287 Loss: 0.1626353414072924, Accuracy: 0.9552333333333332\n",
      "Epoch 288 Loss: 0.16230725735435425, Accuracy: 0.9551333333333332\n",
      "Epoch 289 Loss: 0.16196737025304436, Accuracy: 0.9553333333333333\n",
      "Epoch 290 Loss: 0.16164046751027975, Accuracy: 0.9553833333333333\n",
      "Epoch 291 Loss: 0.16133405372594314, Accuracy: 0.9555999999999999\n",
      "Epoch 292 Loss: 0.16107013769229497, Accuracy: 0.9556499999999999\n",
      "Epoch 293 Loss: 0.16073327685438693, Accuracy: 0.9556833333333332\n",
      "Epoch 294 Loss: 0.16035093041533474, Accuracy: 0.9556666666666666\n",
      "Epoch 295 Loss: 0.1601094110390433, Accuracy: 0.9558833333333332\n",
      "Epoch 296 Loss: 0.15972731307285368, Accuracy: 0.9558333333333333\n",
      "Epoch 297 Loss: 0.15942461840516164, Accuracy: 0.9560166666666666\n",
      "Epoch 298 Loss: 0.15918900867452065, Accuracy: 0.9562999999999999\n",
      "Epoch 299 Loss: 0.1588803580135398, Accuracy: 0.9561833333333332\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "for i in range(num_epochs):\n",
    "    loss, acc = run_epoch(X_train,Y_train,m)\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    print(f\"Epoch {i} Loss: {loss}, Accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6f0330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Figure 1. Neural Network Losses')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(300)),train_losses)\n",
    "plt.plot(list(range(300)),test_losses)\n",
    "plt.xlabel(\"Loss/Cost\")\n",
    "plt.ylabel(\"No. of Epochs\")\n",
    "plt.title(\"Figure 1. Neural Network Performance During Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "014ecb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3026500782381394, 0.0892)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X_test, Y_test, forward_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
