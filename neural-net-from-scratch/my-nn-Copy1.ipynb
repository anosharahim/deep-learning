{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c474a39c",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "## Requirements \n",
    "\n",
    "- A working fully-connected deep neural network from scratch using only numpy.\n",
    "- Includes dense layers, activations, optimizers, loss functions and sigmoid or softmax in case of classification. \n",
    "- Runtime and results on a public dataset.\n",
    "- Documented code that includes brief summary, technical details, and results. \n",
    "\n",
    "## Extensions \n",
    "\n",
    "A comparison of the modelâ€™s runtime and performance with/without each component:\n",
    "- More than 1 optimizer - SGD, Momentum, RMSProp, Adam etc.\n",
    "- Regularization - L2/weight decay, dropout, possibly augmentations if image data etc.\n",
    "- Results on more than 1 dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fab3b7",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "dbbc4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "#load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "88c4b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "650bf4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "m = 60000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = np.array(X[:m]), np.array(X[m:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b8aee2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode mnist data\n",
    "\n",
    "digits =10 \n",
    "examples = 70000\n",
    "\n",
    "y_new = np.array(y, dtype='int32').reshape(-1)\n",
    "y_new = np.eye(10)[y_new]\n",
    "Y_train, Y_test = np.array(y_new[:m]), np.array(y_new[m:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7fa27ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def der_sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def cross_entropy(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Binary cross entropy loss function.\n",
    "    \"\"\"\n",
    "    L = -np.log(np.max(Y_hat * Y, axis=1) + 1e-8) #add small number to avoid zero error\n",
    "    L = np.average(L)\n",
    "    return L\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    \n",
    "    prob = []\n",
    "    for arr in x:\n",
    "        exponentiated = [np.exp(x) for x in arr]\n",
    "        denominator = np.sum(exponentiated)\n",
    "        prob.append([x / denominator for x in exponentiated])\n",
    "    return np.array(prob)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    bools = np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)\n",
    "    return np.average(bools.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3e9d8591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross_entropy(np.array([[0,1,0],[1,0,0]]), np.array([[0.5,0.5,0],[0.2,0.8,0]]))\n",
    "# accuracy([[0,1,0],[0,0,1]], [[0.2,0.8,0],[0.9,0.1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "7c8fb26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch avg. loss: 2.3015792013701866, Accuracy: 0.11146666666666667\n",
      "Epoch avg. loss: 2.301246495414362, Accuracy: 0.11221666666666667\n",
      "Epoch avg. loss: 2.300916533733106, Accuracy: 0.11236666666666667\n",
      "Epoch avg. loss: 2.2996771821842708, Accuracy: 0.11236666666666666\n",
      "Epoch avg. loss: 2.2957895456517226, Accuracy: 0.11275\n",
      "Epoch avg. loss: 2.2844175037371435, Accuracy: 0.14225\n",
      "Epoch avg. loss: 2.2564970810077982, Accuracy: 0.2232833333333333\n",
      "Epoch avg. loss: 2.195971624618307, Accuracy: 0.26676666666666665\n",
      "Epoch avg. loss: 2.074729260626744, Accuracy: 0.32871666666666666\n",
      "Epoch avg. loss: 1.8882151090138786, Accuracy: 0.40323333333333333\n",
      "Epoch avg. loss: 1.6863958417971185, Accuracy: 0.4728333333333334\n",
      "Epoch avg. loss: 1.5076561834939683, Accuracy: 0.5326333333333332\n",
      "Epoch avg. loss: 1.3552210943974825, Accuracy: 0.5841\n",
      "Epoch avg. loss: 1.2283905070954815, Accuracy: 0.6220166666666667\n",
      "Epoch avg. loss: 1.1255527145178261, Accuracy: 0.6647333333333334\n",
      "Epoch avg. loss: 1.0410544134513582, Accuracy: 0.69645\n",
      "Epoch avg. loss: 0.9695926523700954, Accuracy: 0.7257666666666667\n",
      "Epoch avg. loss: 0.9071134858014147, Accuracy: 0.7478333333333333\n",
      "Epoch avg. loss: 0.8518680552106964, Accuracy: 0.7676499999999999\n",
      "Epoch avg. loss: 0.8026127186033835, Accuracy: 0.7843499999999999\n",
      "Epoch avg. loss: 0.7586728067978917, Accuracy: 0.7987833333333334\n",
      "Epoch avg. loss: 0.719841596633818, Accuracy: 0.81\n",
      "Epoch avg. loss: 0.6851032688066664, Accuracy: 0.8199833333333334\n",
      "Epoch avg. loss: 0.6545930283155342, Accuracy: 0.8289500000000001\n",
      "Epoch avg. loss: 0.6274180676143565, Accuracy: 0.8361833333333334\n",
      "Epoch avg. loss: 0.6033637473401007, Accuracy: 0.84215\n",
      "Epoch avg. loss: 0.5818338744658388, Accuracy: 0.8478333333333332\n",
      "Epoch avg. loss: 0.5624870971623556, Accuracy: 0.8527833333333333\n",
      "Epoch avg. loss: 0.5450647042830407, Accuracy: 0.8574\n",
      "Epoch avg. loss: 0.5291030825583045, Accuracy: 0.8612833333333333\n",
      "Epoch avg. loss: 0.5147411418389417, Accuracy: 0.8656333333333335\n",
      "Epoch avg. loss: 0.5014832746360184, Accuracy: 0.8684666666666667\n",
      "Epoch avg. loss: 0.48940938774790227, Accuracy: 0.8717\n",
      "Epoch avg. loss: 0.4783268639794704, Accuracy: 0.8745\n",
      "Epoch avg. loss: 0.4678813496117161, Accuracy: 0.87685\n",
      "Epoch avg. loss: 0.45840971271658393, Accuracy: 0.8791666666666668\n",
      "Epoch avg. loss: 0.44967923702656937, Accuracy: 0.8813\n",
      "Epoch avg. loss: 0.44152805708411574, Accuracy: 0.8824333333333333\n",
      "Epoch avg. loss: 0.43395854580212967, Accuracy: 0.8844666666666667\n",
      "Epoch avg. loss: 0.42687611970487754, Accuracy: 0.8858833333333334\n",
      "Epoch avg. loss: 0.4202176494525699, Accuracy: 0.8876833333333333\n",
      "Epoch avg. loss: 0.4141339755578252, Accuracy: 0.8890166666666667\n",
      "Epoch avg. loss: 0.4081588141472232, Accuracy: 0.8907999999999999\n",
      "Epoch avg. loss: 0.4028428463273061, Accuracy: 0.8916833333333334\n",
      "Epoch avg. loss: 0.39754427850630497, Accuracy: 0.8927999999999999\n",
      "Epoch avg. loss: 0.39275371696049705, Accuracy: 0.89455\n",
      "Epoch avg. loss: 0.38822464164381504, Accuracy: 0.89555\n",
      "Epoch avg. loss: 0.3838308004841426, Accuracy: 0.8967666666666666\n",
      "Epoch avg. loss: 0.3795932122489387, Accuracy: 0.8973666666666665\n",
      "Epoch avg. loss: 0.3756843049036918, Accuracy: 0.898\n",
      "Epoch avg. loss: 0.372008356549267, Accuracy: 0.8991833333333333\n",
      "Epoch avg. loss: 0.3684041565599858, Accuracy: 0.89995\n",
      "Epoch avg. loss: 0.36495834236229036, Accuracy: 0.9009166666666667\n",
      "Epoch avg. loss: 0.36175939277107416, Accuracy: 0.9014833333333333\n",
      "Epoch avg. loss: 0.358606116057084, Accuracy: 0.9026666666666667\n",
      "Epoch avg. loss: 0.35545173605432556, Accuracy: 0.9032666666666667\n",
      "Epoch avg. loss: 0.35257932117453517, Accuracy: 0.9038666666666667\n",
      "Epoch avg. loss: 0.34978283264525395, Accuracy: 0.9049166666666667\n",
      "Epoch avg. loss: 0.3470031537427582, Accuracy: 0.9051333333333333\n",
      "Epoch avg. loss: 0.34446043923235997, Accuracy: 0.9055833333333334\n",
      "Epoch avg. loss: 0.3418799206761466, Accuracy: 0.9064666666666666\n",
      "Epoch avg. loss: 0.33934930385482764, Accuracy: 0.9066666666666667\n",
      "Epoch avg. loss: 0.3370662293667138, Accuracy: 0.9076000000000001\n",
      "Epoch avg. loss: 0.33473310394716227, Accuracy: 0.9081499999999999\n",
      "Epoch avg. loss: 0.33233869269215244, Accuracy: 0.9087333333333334\n",
      "Epoch avg. loss: 0.33027110330977794, Accuracy: 0.9091333333333333\n",
      "Epoch avg. loss: 0.3280945779554202, Accuracy: 0.9094\n",
      "Epoch avg. loss: 0.3259889069541331, Accuracy: 0.9101999999999999\n",
      "Epoch avg. loss: 0.3239272790003474, Accuracy: 0.9106500000000001\n",
      "Epoch avg. loss: 0.3220146420943431, Accuracy: 0.9115666666666666\n",
      "Epoch avg. loss: 0.31996301497245555, Accuracy: 0.9118333333333333\n",
      "Epoch avg. loss: 0.31816530440666574, Accuracy: 0.9123499999999999\n",
      "Epoch avg. loss: 0.31623947253041923, Accuracy: 0.9126166666666665\n",
      "Epoch avg. loss: 0.3143937460266083, Accuracy: 0.9130833333333332\n",
      "Epoch avg. loss: 0.312618358711115, Accuracy: 0.9134333333333333\n",
      "Epoch avg. loss: 0.31090607073623244, Accuracy: 0.9143166666666666\n",
      "Epoch avg. loss: 0.309099174649223, Accuracy: 0.9140166666666667\n",
      "Epoch avg. loss: 0.3074421651276832, Accuracy: 0.9146666666666666\n",
      "Epoch avg. loss: 0.3057471629061882, Accuracy: 0.9151833333333332\n",
      "Epoch avg. loss: 0.30410312882951246, Accuracy: 0.9159833333333334\n",
      "Epoch avg. loss: 0.30245013861261527, Accuracy: 0.9161166666666666\n",
      "Epoch avg. loss: 0.3009861988336553, Accuracy: 0.9168499999999999\n",
      "Epoch avg. loss: 0.2993847206747101, Accuracy: 0.9172666666666667\n",
      "Epoch avg. loss: 0.2978367550487555, Accuracy: 0.9177833333333333\n",
      "Epoch avg. loss: 0.29637419449330854, Accuracy: 0.9180499999999999\n",
      "Epoch avg. loss: 0.29488421226997363, Accuracy: 0.9183499999999999\n",
      "Epoch avg. loss: 0.2933629843077569, Accuracy: 0.9187000000000001\n",
      "Epoch avg. loss: 0.29196922945673864, Accuracy: 0.9190666666666666\n",
      "Epoch avg. loss: 0.29055668876674473, Accuracy: 0.9195333333333333\n",
      "Epoch avg. loss: 0.2891979824017936, Accuracy: 0.9198666666666666\n",
      "Epoch avg. loss: 0.2877908353557485, Accuracy: 0.9202666666666667\n",
      "Epoch avg. loss: 0.28636458481456895, Accuracy: 0.9206833333333332\n",
      "Epoch avg. loss: 0.28511336299312073, Accuracy: 0.9208833333333333\n",
      "Epoch avg. loss: 0.283742796644623, Accuracy: 0.9214333333333333\n",
      "Epoch avg. loss: 0.28248379511651467, Accuracy: 0.9219166666666665\n",
      "Epoch avg. loss: 0.2811303797850551, Accuracy: 0.9221999999999998\n",
      "Epoch avg. loss: 0.27985211107825614, Accuracy: 0.92265\n",
      "Epoch avg. loss: 0.278706616080785, Accuracy: 0.9228333333333333\n",
      "Epoch avg. loss: 0.27737234086723844, Accuracy: 0.9230166666666666\n",
      "Epoch avg. loss: 0.2762083235498654, Accuracy: 0.9235999999999999\n",
      "Epoch avg. loss: 0.27496897044872753, Accuracy: 0.9237333333333332\n",
      "Epoch avg. loss: 0.27376551098464846, Accuracy: 0.9243499999999999\n",
      "Epoch avg. loss: 0.2725016271201006, Accuracy: 0.9243999999999999\n",
      "Epoch avg. loss: 0.2714380280610858, Accuracy: 0.9245333333333332\n",
      "Epoch avg. loss: 0.27015659435536643, Accuracy: 0.9251833333333332\n",
      "Epoch avg. loss: 0.269196129552042, Accuracy: 0.9253666666666666\n",
      "Epoch avg. loss: 0.26790497592236645, Accuracy: 0.9256\n",
      "Epoch avg. loss: 0.2668738444968678, Accuracy: 0.9259333333333333\n",
      "Epoch avg. loss: 0.26580747260926024, Accuracy: 0.9264\n",
      "Epoch avg. loss: 0.2646284443961158, Accuracy: 0.9263666666666666\n",
      "Epoch avg. loss: 0.26357693206595967, Accuracy: 0.9266499999999999\n",
      "Epoch avg. loss: 0.2625430912024317, Accuracy: 0.9268666666666665\n",
      "Epoch avg. loss: 0.26148006665619095, Accuracy: 0.9273333333333332\n",
      "Epoch avg. loss: 0.26039059611955967, Accuracy: 0.9277333333333332\n",
      "Epoch avg. loss: 0.2594660649443084, Accuracy: 0.9282833333333333\n",
      "Epoch avg. loss: 0.25834393162203323, Accuracy: 0.9282333333333332\n",
      "Epoch avg. loss: 0.2573966951820519, Accuracy: 0.9282999999999999\n",
      "Epoch avg. loss: 0.2563308712715858, Accuracy: 0.9287499999999999\n",
      "Epoch avg. loss: 0.25536364424638947, Accuracy: 0.9287499999999999\n",
      "Epoch avg. loss: 0.25440198723186463, Accuracy: 0.9291499999999998\n",
      "Epoch avg. loss: 0.2533610022177748, Accuracy: 0.9294333333333333\n",
      "Epoch avg. loss: 0.2523059327076816, Accuracy: 0.9295166666666667\n",
      "Epoch avg. loss: 0.2514999807440998, Accuracy: 0.9298666666666665\n",
      "Epoch avg. loss: 0.25052415186547977, Accuracy: 0.9303166666666666\n",
      "Epoch avg. loss: 0.24965594580144732, Accuracy: 0.9301833333333333\n",
      "Epoch avg. loss: 0.24858627448011122, Accuracy: 0.9306333333333332\n",
      "Epoch avg. loss: 0.24780488085827876, Accuracy: 0.9310833333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch avg. loss: 0.24687736018685258, Accuracy: 0.9308833333333333\n",
      "Epoch avg. loss: 0.2458829991429177, Accuracy: 0.9312999999999999\n",
      "Epoch avg. loss: 0.24500876868796262, Accuracy: 0.9317000000000001\n",
      "Epoch avg. loss: 0.2441874212053851, Accuracy: 0.9317833333333333\n",
      "Epoch avg. loss: 0.24323241793811848, Accuracy: 0.93245\n",
      "Epoch avg. loss: 0.2424267993941037, Accuracy: 0.9323833333333332\n",
      "Epoch avg. loss: 0.24159701649125184, Accuracy: 0.9327333333333332\n",
      "Epoch avg. loss: 0.24071642303967145, Accuracy: 0.9330999999999999\n",
      "Epoch avg. loss: 0.2398787603143896, Accuracy: 0.9332166666666666\n",
      "Epoch avg. loss: 0.23905992832396192, Accuracy: 0.9332666666666667\n",
      "Epoch avg. loss: 0.23817838277618827, Accuracy: 0.9336833333333333\n",
      "Epoch avg. loss: 0.23722781986469144, Accuracy: 0.9341666666666665\n",
      "Epoch avg. loss: 0.23665003156516837, Accuracy: 0.9343166666666666\n",
      "Epoch avg. loss: 0.23577971538294018, Accuracy: 0.9344333333333332\n",
      "Epoch avg. loss: 0.23496097972068128, Accuracy: 0.9346999999999999\n",
      "Epoch avg. loss: 0.23419794468012345, Accuracy: 0.9346499999999999\n",
      "Epoch avg. loss: 0.23327929537571543, Accuracy: 0.9350333333333333\n",
      "Epoch avg. loss: 0.23261880137104093, Accuracy: 0.9351999999999999\n",
      "Epoch avg. loss: 0.23177486469515338, Accuracy: 0.9354666666666666\n",
      "Epoch avg. loss: 0.2311386529243519, Accuracy: 0.9357333333333332\n",
      "Epoch avg. loss: 0.23026376230943804, Accuracy: 0.9360166666666666\n",
      "Epoch avg. loss: 0.22946947931065556, Accuracy: 0.9361\n",
      "Epoch avg. loss: 0.22876702538970883, Accuracy: 0.9364166666666666\n",
      "Epoch avg. loss: 0.2281442501543095, Accuracy: 0.9364333333333332\n",
      "Epoch avg. loss: 0.22731615034226094, Accuracy: 0.9366833333333333\n",
      "Epoch avg. loss: 0.22656881587815575, Accuracy: 0.9370666666666666\n",
      "Epoch avg. loss: 0.22577083300921538, Accuracy: 0.9373666666666666\n",
      "Epoch avg. loss: 0.22520807501060244, Accuracy: 0.9374333333333332\n",
      "Epoch avg. loss: 0.22432319508324464, Accuracy: 0.9377333333333333\n",
      "Epoch avg. loss: 0.223761226975066, Accuracy: 0.9381166666666666\n",
      "Epoch avg. loss: 0.2230738301607537, Accuracy: 0.9382166666666666\n",
      "Epoch avg. loss: 0.2222059727792185, Accuracy: 0.9383166666666666\n",
      "Epoch avg. loss: 0.22167739727573013, Accuracy: 0.9386666666666665\n",
      "Epoch avg. loss: 0.22092755638597594, Accuracy: 0.939\n",
      "Epoch avg. loss: 0.22027650219746045, Accuracy: 0.9391499999999998\n",
      "Epoch avg. loss: 0.2195315511936348, Accuracy: 0.93925\n",
      "Epoch avg. loss: 0.2188558465856699, Accuracy: 0.9394999999999999\n",
      "Epoch avg. loss: 0.21827009734875719, Accuracy: 0.9394666666666666\n",
      "Epoch avg. loss: 0.21763799016277635, Accuracy: 0.9395666666666667\n",
      "Epoch avg. loss: 0.2169245796996707, Accuracy: 0.9400999999999999\n",
      "Epoch avg. loss: 0.21631833840725917, Accuracy: 0.9400999999999999\n",
      "Epoch avg. loss: 0.215690932680934, Accuracy: 0.9401666666666666\n",
      "Epoch avg. loss: 0.2148663685336113, Accuracy: 0.9406999999999999\n",
      "Epoch avg. loss: 0.21432410847980693, Accuracy: 0.9406333333333332\n",
      "Epoch avg. loss: 0.21376949094764156, Accuracy: 0.9407833333333333\n",
      "Epoch avg. loss: 0.21315093212802436, Accuracy: 0.9411499999999998\n",
      "Epoch avg. loss: 0.21245251623599523, Accuracy: 0.9412333333333334\n",
      "Epoch avg. loss: 0.2118906445880625, Accuracy: 0.9412833333333331\n",
      "Epoch avg. loss: 0.2112327869397311, Accuracy: 0.94155\n",
      "Epoch avg. loss: 0.21054156427610513, Accuracy: 0.9416333333333333\n",
      "Epoch avg. loss: 0.21004735642398747, Accuracy: 0.9420666666666666\n",
      "Epoch avg. loss: 0.20935513002324466, Accuracy: 0.9421666666666665\n",
      "Epoch avg. loss: 0.20887156000434323, Accuracy: 0.9424833333333332\n",
      "Epoch avg. loss: 0.20817293687617938, Accuracy: 0.9423999999999999\n",
      "Epoch avg. loss: 0.20768787253426996, Accuracy: 0.9425166666666667\n",
      "Epoch avg. loss: 0.20710060904416427, Accuracy: 0.9425833333333332\n",
      "Epoch avg. loss: 0.20641686666697961, Accuracy: 0.9428666666666665\n",
      "Epoch avg. loss: 0.2059701287736821, Accuracy: 0.9429833333333334\n",
      "Epoch avg. loss: 0.20532363858380132, Accuracy: 0.9434833333333332\n",
      "Epoch avg. loss: 0.20471550693022755, Accuracy: 0.9433666666666666\n",
      "Epoch avg. loss: 0.20421337873140596, Accuracy: 0.9436999999999999\n",
      "Epoch avg. loss: 0.2036470107228402, Accuracy: 0.9437166666666665\n",
      "Epoch avg. loss: 0.2030505734698356, Accuracy: 0.9441166666666666\n",
      "Epoch avg. loss: 0.2025968909584236, Accuracy: 0.9438666666666665\n",
      "Epoch avg. loss: 0.20192937449653459, Accuracy: 0.9443833333333332\n",
      "Epoch avg. loss: 0.20145868227672895, Accuracy: 0.9443833333333332\n",
      "Epoch avg. loss: 0.2009267486482293, Accuracy: 0.9446333333333333\n",
      "Epoch avg. loss: 0.20038493920801645, Accuracy: 0.94475\n",
      "Epoch avg. loss: 0.19978341717317413, Accuracy: 0.9446333333333332\n",
      "Epoch avg. loss: 0.19929782338073532, Accuracy: 0.9447166666666665\n",
      "Epoch avg. loss: 0.1987888296674011, Accuracy: 0.9450833333333333\n",
      "Epoch avg. loss: 0.19820910743912895, Accuracy: 0.9450333333333333\n",
      "Epoch avg. loss: 0.19774852404847676, Accuracy: 0.9452833333333333\n",
      "Epoch avg. loss: 0.19716740293987772, Accuracy: 0.9455333333333333\n",
      "Epoch avg. loss: 0.19675549534971173, Accuracy: 0.9455833333333332\n",
      "Epoch avg. loss: 0.1961739001699007, Accuracy: 0.94575\n",
      "Epoch avg. loss: 0.19559563784203823, Accuracy: 0.9458999999999999\n",
      "Epoch avg. loss: 0.19521123336308535, Accuracy: 0.9459833333333333\n",
      "Epoch avg. loss: 0.1947149982290323, Accuracy: 0.9459166666666665\n",
      "Epoch avg. loss: 0.1941460067070968, Accuracy: 0.9463499999999999\n",
      "Epoch avg. loss: 0.19365798581557192, Accuracy: 0.9467166666666664\n",
      "Epoch avg. loss: 0.19315469417256936, Accuracy: 0.9468666666666665\n",
      "Epoch avg. loss: 0.1926988331205926, Accuracy: 0.94655\n",
      "Epoch avg. loss: 0.19224760322184328, Accuracy: 0.9468999999999999\n",
      "Epoch avg. loss: 0.19178720133095264, Accuracy: 0.9469666666666665\n",
      "Epoch avg. loss: 0.19130871498885077, Accuracy: 0.9470666666666665\n",
      "Epoch avg. loss: 0.19065927743674915, Accuracy: 0.9471166666666666\n",
      "Epoch avg. loss: 0.1904284541593053, Accuracy: 0.9472666666666666\n",
      "Epoch avg. loss: 0.18987224016981083, Accuracy: 0.9472499999999998\n",
      "Epoch avg. loss: 0.1894245277780735, Accuracy: 0.9474166666666665\n",
      "Epoch avg. loss: 0.18882253993892653, Accuracy: 0.9476833333333332\n",
      "Epoch avg. loss: 0.18845190883130788, Accuracy: 0.9478333333333333\n",
      "Epoch avg. loss: 0.18799398287218447, Accuracy: 0.9479499999999998\n",
      "Epoch avg. loss: 0.18751071519358367, Accuracy: 0.9482333333333334\n",
      "Epoch avg. loss: 0.18705046366824227, Accuracy: 0.9483833333333332\n",
      "Epoch avg. loss: 0.18661219980162433, Accuracy: 0.94805\n",
      "Epoch avg. loss: 0.1861653148772299, Accuracy: 0.9486499999999999\n",
      "Epoch avg. loss: 0.18568954184045577, Accuracy: 0.9484499999999998\n",
      "Epoch avg. loss: 0.18532623457840297, Accuracy: 0.9487166666666665\n",
      "Epoch avg. loss: 0.18486817192514574, Accuracy: 0.9487333333333332\n",
      "Epoch avg. loss: 0.1843832817223927, Accuracy: 0.9486833333333333\n",
      "Epoch avg. loss: 0.18402368280365455, Accuracy: 0.9487333333333333\n",
      "Epoch avg. loss: 0.18344585788782145, Accuracy: 0.9492166666666665\n",
      "Epoch avg. loss: 0.1831719454389156, Accuracy: 0.9492499999999999\n",
      "Epoch avg. loss: 0.18265505342598887, Accuracy: 0.9494833333333333\n",
      "Epoch avg. loss: 0.18230837967028318, Accuracy: 0.9492833333333331\n",
      "Epoch avg. loss: 0.18186625479815918, Accuracy: 0.9497666666666666\n",
      "Epoch avg. loss: 0.18142249499144222, Accuracy: 0.9497499999999999\n",
      "Epoch avg. loss: 0.1810101972487557, Accuracy: 0.9500166666666664\n",
      "Epoch avg. loss: 0.1805284485606502, Accuracy: 0.9500333333333333\n",
      "Epoch avg. loss: 0.18017773402052598, Accuracy: 0.9500499999999998\n",
      "Epoch avg. loss: 0.17981863438324636, Accuracy: 0.9500666666666666\n",
      "Epoch avg. loss: 0.17925721652640753, Accuracy: 0.9502166666666666\n",
      "Epoch avg. loss: 0.17896985512918678, Accuracy: 0.9505499999999999\n",
      "Epoch avg. loss: 0.17852135317083856, Accuracy: 0.9506333333333332\n",
      "Epoch avg. loss: 0.178136954141391, Accuracy: 0.9507333333333332\n",
      "Epoch avg. loss: 0.17778661205474314, Accuracy: 0.9508999999999999\n",
      "Epoch avg. loss: 0.17734269463523764, Accuracy: 0.9509666666666666\n",
      "Epoch avg. loss: 0.17701682548828823, Accuracy: 0.9510166666666665\n",
      "Epoch avg. loss: 0.17660368826499473, Accuracy: 0.9508833333333332\n",
      "Epoch avg. loss: 0.17614681558719497, Accuracy: 0.9513833333333332\n",
      "Epoch avg. loss: 0.17576743337200504, Accuracy: 0.9511999999999998\n",
      "Epoch avg. loss: 0.1753942423286041, Accuracy: 0.9515333333333333\n",
      "Epoch avg. loss: 0.17491326676300373, Accuracy: 0.9514999999999999\n",
      "Epoch avg. loss: 0.17473268053360347, Accuracy: 0.9515999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch avg. loss: 0.17421742673965018, Accuracy: 0.9514833333333332\n",
      "Epoch avg. loss: 0.17388919793036245, Accuracy: 0.9521833333333332\n",
      "Epoch avg. loss: 0.17349638028181563, Accuracy: 0.9516833333333333\n",
      "Epoch avg. loss: 0.17311378071902686, Accuracy: 0.9518666666666665\n",
      "Epoch avg. loss: 0.17286540770065711, Accuracy: 0.9519666666666667\n",
      "Epoch avg. loss: 0.17237987829969853, Accuracy: 0.9519499999999999\n",
      "Epoch avg. loss: 0.1720233602056506, Accuracy: 0.9523833333333332\n",
      "Epoch avg. loss: 0.17161636150018705, Accuracy: 0.9525833333333332\n",
      "Epoch avg. loss: 0.1713423251722635, Accuracy: 0.9524499999999998\n",
      "Epoch avg. loss: 0.17095461937990414, Accuracy: 0.9527833333333332\n",
      "Epoch avg. loss: 0.1705796568661176, Accuracy: 0.9525999999999999\n",
      "Epoch avg. loss: 0.17021853780744703, Accuracy: 0.9530166666666665\n",
      "Epoch avg. loss: 0.1697922334916617, Accuracy: 0.9528999999999999\n",
      "Epoch avg. loss: 0.16951415836203973, Accuracy: 0.9531833333333332\n",
      "Epoch avg. loss: 0.16907630207069918, Accuracy: 0.9530666666666666\n",
      "Epoch avg. loss: 0.1688280925243672, Accuracy: 0.9530833333333333\n",
      "Epoch avg. loss: 0.16849568006988228, Accuracy: 0.9533166666666666\n",
      "Epoch avg. loss: 0.1680890214754391, Accuracy: 0.9532166666666667\n",
      "Epoch avg. loss: 0.16774786468369535, Accuracy: 0.9534166666666665\n",
      "Epoch avg. loss: 0.1674600506261927, Accuracy: 0.9535333333333332\n",
      "Epoch avg. loss: 0.1670800514499443, Accuracy: 0.9536666666666666\n",
      "Epoch avg. loss: 0.16676297654366917, Accuracy: 0.9536999999999998\n",
      "Epoch avg. loss: 0.16638519616768677, Accuracy: 0.9540833333333333\n",
      "Epoch avg. loss: 0.16603695869885182, Accuracy: 0.9536999999999999\n",
      "Epoch avg. loss: 0.16568928087906723, Accuracy: 0.9541666666666665\n",
      "Epoch avg. loss: 0.1654069201433225, Accuracy: 0.9542499999999999\n",
      "Epoch avg. loss: 0.1650668174150715, Accuracy: 0.9542499999999999\n",
      "Epoch avg. loss: 0.16472901540351143, Accuracy: 0.9544166666666665\n",
      "Epoch avg. loss: 0.1644195263354477, Accuracy: 0.9543999999999999\n",
      "Epoch avg. loss: 0.16406309268677877, Accuracy: 0.9545166666666667\n",
      "Epoch avg. loss: 0.1637612944899109, Accuracy: 0.95455\n",
      "Epoch avg. loss: 0.16343716430973035, Accuracy: 0.9545166666666666\n",
      "Epoch avg. loss: 0.1630841438375292, Accuracy: 0.95475\n",
      "Epoch avg. loss: 0.16281113107997333, Accuracy: 0.9547499999999999\n",
      "Epoch avg. loss: 0.16250281733088043, Accuracy: 0.9549499999999999\n",
      "Epoch avg. loss: 0.1621512940981971, Accuracy: 0.9552666666666665\n",
      "Epoch avg. loss: 0.1618204510892939, Accuracy: 0.9553499999999998\n",
      "Epoch avg. loss: 0.16155033519812118, Accuracy: 0.9550666666666666\n",
      "Epoch avg. loss: 0.16113043981013822, Accuracy: 0.9549999999999997\n",
      "Epoch avg. loss: 0.16088876480533534, Accuracy: 0.9554833333333331\n",
      "Epoch avg. loss: 0.16060685640945235, Accuracy: 0.9553999999999999\n",
      "Epoch avg. loss: 0.16023733905009988, Accuracy: 0.9553666666666666\n",
      "Epoch avg. loss: 0.15994404366102105, Accuracy: 0.9555833333333332\n",
      "Epoch avg. loss: 0.1596300998101495, Accuracy: 0.9556999999999999\n",
      "Epoch avg. loss: 0.15935906645713394, Accuracy: 0.9557333333333332\n",
      "Epoch avg. loss: 0.15904810646835815, Accuracy: 0.9556499999999999\n",
      "Epoch avg. loss: 0.15875233050054224, Accuracy: 0.9559499999999999\n",
      "Epoch avg. loss: 0.1583650874189568, Accuracy: 0.9561499999999998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size = 784\n",
    "learning_rate = 5\n",
    "batch_size = 60\n",
    "\n",
    "#Initialization: weights and biases\n",
    "w1 = 0.001 * np.random.randn(input_size,64)\n",
    "w2 = 0.001 * np.random.randn(64,10)\n",
    "b1 = np.zeros(64)\n",
    "b2 = np.zeros(10)\n",
    "\n",
    "def gradient_descent(X, y, forward_only=False):\n",
    "    \n",
    "    global w2\n",
    "    global b2\n",
    "    global w1\n",
    "    global b1\n",
    "    z1 = np.dot(X,w1) + b1\n",
    "    s1 = sigmoid(z1)\n",
    "    z2 = np.dot(s1,w2) + b2 \n",
    "    y_hat = softmax(z2) #these are probabilities\n",
    "    L = cross_entropy(y,y_hat) # compute loss using model output and true labels\n",
    "    acc = accuracy(y, y_hat)\n",
    "    \n",
    "    ## Backward propagation ## \n",
    "    \n",
    "    if forward_only:\n",
    "        return L, acc\n",
    "    \n",
    "    #Gradients for Layer 2\n",
    "    d_z2 = y_hat - y #\n",
    "    d_w2 = s1.T.dot(d_z2)\n",
    "    d_b2 = np.sum(d_z2, axis=0)\n",
    "    d_s1 = der_sigmoid(z1) * d_z2.dot(w2.T)\n",
    "    \n",
    "    #Gradients for Layer 1\n",
    "    d_w1 = X.T.dot(d_s1)\n",
    "    d_b1 = np.sum(d_w1, axis=0)\n",
    "    \n",
    "    # Use gradients to update weights and biases\n",
    "    w2 = (w2 - (learning_rate * (d_w2 / m)))\n",
    "    b2 = (b2 - (learning_rate * (d_b2 / m)))\n",
    "    w1 = (w1 - (learning_rate * (d_w1 / m)))\n",
    "    b1 = (b1 - (learning_rate * (d_b1 / m)))\n",
    "    return L, acc\n",
    "\n",
    "\n",
    "def run_epoch(X, y):\n",
    "    \n",
    "    index = np.arange(m)\n",
    "    np.random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    batch_count = m / batch_size\n",
    "    X_batches = np.array_split(X, batch_count)\n",
    "    y_batches = np.array_split(y, batch_count)\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    for i in range(len(X_batches)):\n",
    "        loss, acc = gradient_descent(X_batches[i], y_batches[i])\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "    return np.average(losses), np.average(accs)\n",
    "\n",
    "num_epochs = 300\n",
    "for i in range(num_epochs):\n",
    "    loss, acc = run_epoch(X_train,Y_train)\n",
    "    print(f\"Epoch {i} Loss: {}, Accuracy: {}\")\n",
    "\n",
    "#cross validation \n",
    "#test set \n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
