{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c474a39c",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "## Requirements \n",
    "\n",
    "- A working fully-connected deep neural network from scratch using only numpy.\n",
    "- Includes dense layers, activations, optimizers, loss functions and sigmoid or softmax in case of classification. \n",
    "- Runtime and results on a public dataset.\n",
    "- Documented code that includes brief summary, technical details, and results. \n",
    "\n",
    "## Extensions \n",
    "\n",
    "A comparison of the modelâ€™s runtime and performance with/without each component:\n",
    "- More than 1 optimizer - SGD, Momentum, RMSProp, Adam etc.\n",
    "- Regularization - L2/weight decay, dropout, possibly augmentations if image data etc.\n",
    "- Results on more than 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "#load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "x = mnist.data\n",
    "y = mnist.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c07c9",
   "metadata": {},
   "source": [
    "Forward pass on a single example:\n",
    "$\\hat{y}=\\sigma(w^Tx+b)$ \\\n",
    "Sigmoid Function:\n",
    "$\\sigma = \\frac{1}{1+e^{-z}}$ \\\n",
    "In backpropagation, we need to compute: $\\frac{\\partial L}{\\partial w_j}$ i.e. we need to know how the cost changes with respect to each component of the weight matrix. In other words, we need to know how sensitive the cost function is to each of the components of the weight matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668ae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define activation functions\n",
    "def sigmoid(A):\n",
    "    return 1 / (1 + np.exp(-A))\n",
    "\n",
    "#define loss functions\n",
    "def cross_entropy(Y, Y_hat):\n",
    "    n = Y.shape[1] #-- this is not bias, call it n\n",
    "    #softmax :) \n",
    "    L = -(1./n) * (np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
    "    return L \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33af0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one input layer\n",
    "#two hidden layers\n",
    "#one output layer\n",
    "\n",
    "#MNIST -- 70000 images of 28x28 pixels and 10 classes\n",
    "#stochastic gradient descent \n",
    "\n",
    "#softmax -- vector of 10 values with probability of belonging to said class\n",
    "class nn:\n",
    "    def __init__(self,x,y,hidden_size = 4, num_classes = 10):\n",
    "        self.input = x #inputs \n",
    "        self.y = y #label (one hot encode)  \n",
    "        self.output = np.zeros(num_classes) #predicted one-hot \n",
    "        #weights initialization (randomly for now)\n",
    "        self.weights1 = np.random.rand(self.input.shape,hidden_size) \n",
    "        #deep layer has size of 4 -put in va\n",
    "        self.weights2 = np.random.rand(hidden_size,num_classes)  \n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1)) #get weighted sum of w.x\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2)) \n",
    "        self.loss = cross_entropy(self.y,self.output)\n",
    "    \n",
    "    def backpropagation(self): \n",
    "        #get derivative of loss function with respect to weights1 and weights2\n",
    "        #weights1_deriv = np.dot(self.layer1.T, (2*(self.y - self.output) * derivativeoflossfunc\n",
    "        #weights2_deriv = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * derivativeoflossfunc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa27ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
