{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c474a39c",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "In this project, I have built a fully-connected neural network from scratch using only Numpy to perform multi-class classification on the MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fab3b7",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "I used the MNIST dataset of 70,000 handwritten digit images from sklearn and split it into training (45,000 images), validation (15,000) and test set (10,000 images). The class labels are one-hot encoded in order for the neural network to interpret categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbbc4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## load MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "\n",
    "X = X /255 #normalize for simplicity \n",
    "\n",
    "#split into test, validation, and train sets \n",
    "m = 45000\n",
    "m_val = 15000\n",
    "m_test = 10000\n",
    "X_train, X_val, X_test = np.array(X[:m]), np.array(X[m:m+m_val]), np.array(X[m+m_val:])\n",
    "\n",
    "#one-hot encode mnist labels for classification \n",
    "digits =10 \n",
    "examples = 70000\n",
    "y_new = np.array(y, dtype='int32').reshape(-1)\n",
    "y_new = np.eye(10)[y_new]\n",
    "Y_train, Y_val, Y_test = np.array(y_new[:m]), np.array(y_new[m:m+m_val]), np.array(y_new[m+m_val:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c1c13",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "\n",
    "#### Layers \n",
    "The neural network has 2 fully-connected layers. The first layer i.e. input has 784 nodes of pixels from the MNIST images, which are connected to the first hidden layer, containing 64 nodes, and the output layer, which has 10 nodes for the 10 mnist digits to be classified. \n",
    "\n",
    "#### Activation Function \n",
    "For the activation function, I have used a sigmoid function which keeps the output of each neuron between 0 and 1:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}} $ \n",
    "\n",
    "Since we are doing multi-class classification, I also used a Softmax function on the final layer. Softmax is a takes a vector as input and outputs a probability distribution whose total sums to one. As such , it gives the probability of an input belonging to any particular class. \n",
    "\n",
    "$S(\\vec{x})_i = \\frac{e^{x_i}}{\\Sigma_{j=1}^{K}e^{x_j}} $\n",
    "\n",
    "#### Loss Functions \n",
    "We used a binary cross-entropy function as our loss function in conjunction with softmax. Where softmax outputs probabilities, cross entropy takes those probabilities and measures their distance from the truth value. \n",
    "\n",
    "$L(Y,\\hat{Y})=-\\frac{1}{m}(y^i log(\\hat{y}^i+ (1-y)log(1-\\hat{y}^i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa27ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions for neural network \n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def der_sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def cross_entropy(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Binary cross entropy loss function.\n",
    "    \"\"\"\n",
    "    L = -np.log(np.max(Y_hat * Y, axis=1) + 1e-8) #add small number to avoid zero error\n",
    "    L = np.average(L)\n",
    "    return L\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Computes softmax values for each sets of scores in x.\"\"\"\n",
    "    prob = []\n",
    "    for arr in x:\n",
    "        exponentiated = [np.exp(x) for x in arr]\n",
    "        denominator = np.sum(exponentiated)\n",
    "        prob.append([x / denominator for x in exponentiated])\n",
    "    return np.array(prob)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    '''Computes accuracy of model preDictions .\n",
    "    '''\n",
    "    bools = np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)\n",
    "    return np.average(bools.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016434f7",
   "metadata": {},
   "source": [
    "### Gradient Descent \n",
    "\n",
    "For the gradient descent step, I have randomly initialized weights to be really small numbers, and biases to be zero. Weights cannot be initialized to zero because it needs to be multiplied by the neuron's value, and it would not start training. On the other hand, biases can be initialized to 0 because they just need to be added to the neuron's output. In the function below, weights and biases are globally initialized so that the updated values can be saved and reused for testing. \n",
    "\n",
    "I started out with batch gradient descent, and at a learning rate of 1 it was still learning very slowly since it was performing the entire gradient descent for the entire dataset each time. In order to improve the learning, I implemented mini-batch gradient descent, as well as increased the learning rate to 3, which significantly improved performance on both training and validation set.  \n",
    "\n",
    "#### Forward Pass\n",
    "In forward pass, the input values are fed through the neural network where weights w are multiplied by the input values X along with adding the bias. This is done for each layer as per the following formulas. \n",
    "\n",
    "$ \\hat{y} = \\sigma (z) $\\\n",
    "$ z = w^T x +b $ \\\n",
    "$ \\sigma(z) = \\frac{1}{1+e^{-z}} $ \n",
    "\n",
    "\n",
    "#### Backward Pass \n",
    "In backward pass, the main thing we need to is find out how sensitive the loss is to different components of the weights and bias matrices. In order to do this, we take the derivative of the loss function using the chain rule as follows: \n",
    "\n",
    "$L(Y,\\hat{Y})=-\\frac{1}{m}(y^i \\log(\\hat{y}^i+ (1-y)\\log(1-\\hat{y}^i))$\n",
    "\n",
    "We need to find the derivative of the loss function with respect of biases and weights, for which we apply the chain rule as follows: \n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_j}=\\frac{\\partial L}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial z}.\\frac{\\partial z}{\\partial w_j}$\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial b} =\\frac{1}{m} \\Sigma_{i=1}^m(\\hat{y}^i-y^i)  $ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\hat{y}} = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}$\n",
    "\n",
    "$\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial w_j} = \\frac{\\partial (w_0x_0+w_1 x_1+w_2 x_2+....+w_n x_n+b)}{\\partial w_j}$\n",
    "\n",
    "This is the main logic behind the gradient descent function. Using this chain rule, we get the gradients of the loss function with respect to the biases and weights, which are then used to update the weights and biases. Full derivations for the above equations can be found on Jonathan Weisberg's blogpost on [Building Neural Networks from Scratch](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8fb26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 #the input layer has 784 pixels as nodes \n",
    "learning_rate = 5  #higher learning rate to speed up training\n",
    "batch_size = 60 #specify batch size for mini-batch gradient descent \n",
    "\n",
    "#Initialization: weights and biases\n",
    "\n",
    "#weights can be very small non-zero random numbers \n",
    "w1 = 0.001 * np.random.randn(input_size,64) \n",
    "w2 = 0.001 * np.random.randn(64,10)\n",
    "#biases can be zeros at the start \n",
    "b1 = np.zeros(64) \n",
    "b2 = np.zeros(10)\n",
    "\n",
    "def gradient_descent(X, y, forward_only=False):\n",
    "    \"\"\"Performs gradient descent on MNIST data to compute loss and accuracy.    \n",
    "    \"\"\"\n",
    "\n",
    "    #weights and biases are globally initialized so they can be saved for later\n",
    "    global w2\n",
    "    global b2\n",
    "    global w1\n",
    "    global b1\n",
    "    \n",
    "    #### Forward Propagation ####\n",
    "    \n",
    "    z1 = np.dot(X,w1) + b1 #multipy weights by input nodes and add bias \n",
    "    s1 = sigmoid(z1) #put it through the activation function \n",
    "    z2 = np.dot(s1,w2) + b2  #multipy weights by previous nodes and add bias \n",
    "    y_hat = softmax(z2) #softmax outputs probabilities as the model output \n",
    "    L = cross_entropy(y,y_hat) #compute loss using model output and true labels\n",
    "    acc = accuracy(y, y_hat) #compute accuracy of predictions\n",
    "    \n",
    "    if forward_only: #for testing, only forward propagate  \n",
    "        return L, acc\n",
    "    \n",
    "    #### Backward propagation ####\n",
    "    \n",
    "    #Apply chain rule on each layer to see how cost changes with respect to \n",
    "    #each component of weights matrix and each bias \n",
    "    \n",
    "    #Gradients for Layer 2 \n",
    "    d_z2 = y_hat - y \n",
    "    d_w2 = s1.T.dot(d_z2) # \n",
    "    d_b2 = np.sum(d_z2, axis=0)\n",
    "    d_s1 = der_sigmoid(z1) * d_z2.dot(w2.T)\n",
    "    \n",
    "    #Gradients for Layer 1\n",
    "    d_w1 = X.T.dot(d_s1)\n",
    "    d_b1 = np.sum(d_w1, axis=0)\n",
    "    \n",
    "    #Use gradients to update weights and biases\n",
    "    w2 = (w2 - (learning_rate * (d_w2 / m)))\n",
    "    b2 = (b2 - (learning_rate * (d_b2 / m)))\n",
    "    w1 = (w1 - (learning_rate * (d_w1 / m)))\n",
    "    b1 = (b1 - (learning_rate * (d_b1 / m)))\n",
    "    \n",
    "    return L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a3bc6",
   "metadata": {},
   "source": [
    "### Training and Performance on Validation/Test Set\n",
    "\n",
    "When applying this neural network to the validation set, I set backpropagation off to compute loss and accuracy. Using mini-batches helped increase the speed of training and did not require holding the entire dataset in memory, as opposed to batch gradient descent. However, mini-batch still runs the risk of getting trapped at local minima. Moreover, increasing the learning rate to 5 resulted in 95% accuracy after 230 epochs, as opposed to staying at 94% after 300 epochs at a learning rate of 4. The learning can be improved in the future by adding other optimization methods such as by incrementally and adaptively increasing learning rate as needed, so as to not overshoot. So far, the neural network can predict handwritten digits with about 94.9% accuracy on the validation data as well as the test data, which shows evidence for little overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de4e2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 2.3014495498589502, Training Accuracy: 0.1132888888888889 Validation Accuracy 0.10760000000000002\n",
      "Epoch 1 Loss: 2.3011230239260434, Training Accuracy: 0.11373333333333333 Validation Accuracy 0.1076\n",
      "Epoch 2 Loss: 2.3007193336356995, Training Accuracy: 0.11395555555555556 Validation Accuracy 0.10760000000000002\n",
      "Epoch 3 Loss: 2.2995404739981065, Training Accuracy: 0.11395555555555556 Validation Accuracy 0.1076\n",
      "Epoch 4 Loss: 2.295965058502212, Training Accuracy: 0.11395555555555556 Validation Accuracy 0.1076\n",
      "Epoch 5 Loss: 2.284899124003581, Training Accuracy: 0.13573333333333332 Validation Accuracy 0.12546666666666664\n",
      "Epoch 6 Loss: 2.2573605976727036, Training Accuracy: 0.20919999999999997 Validation Accuracy 0.22773333333333334\n",
      "Epoch 7 Loss: 2.196522397421411, Training Accuracy: 0.2723111111111111 Validation Accuracy 0.303\n",
      "Epoch 8 Loss: 2.0724876007009394, Training Accuracy: 0.3322888888888889 Validation Accuracy 0.3809333333333333\n",
      "Epoch 9 Loss: 1.8813355990766825, Training Accuracy: 0.4159555555555556 Validation Accuracy 0.4496\n",
      "Epoch 10 Loss: 1.6764921866849998, Training Accuracy: 0.48715555555555556 Validation Accuracy 0.5289333333333334\n",
      "Epoch 11 Loss: 1.4980823458074752, Training Accuracy: 0.5449555555555555 Validation Accuracy 0.5816666666666668\n",
      "Epoch 12 Loss: 1.349203216554974, Training Accuracy: 0.5932222222222222 Validation Accuracy 0.6159333333333334\n",
      "Epoch 13 Loss: 1.2266337354981574, Training Accuracy: 0.6331111111111112 Validation Accuracy 0.6711333333333334\n",
      "Epoch 14 Loss: 1.1258827967269875, Training Accuracy: 0.6743333333333333 Validation Accuracy 0.7058666666666665\n",
      "Epoch 15 Loss: 1.0416255980524143, Training Accuracy: 0.7060666666666667 Validation Accuracy 0.733\n",
      "Epoch 16 Loss: 0.9691939900607881, Training Accuracy: 0.7320222222222224 Validation Accuracy 0.76\n",
      "Epoch 17 Loss: 0.9054892098261595, Training Accuracy: 0.7555111111111111 Validation Accuracy 0.7811333333333335\n",
      "Epoch 18 Loss: 0.8488882232358322, Training Accuracy: 0.7762222222222221 Validation Accuracy 0.7963999999999999\n",
      "Epoch 19 Loss: 0.7985222401762873, Training Accuracy: 0.7907555555555554 Validation Accuracy 0.8122666666666667\n",
      "Epoch 20 Loss: 0.7541512146354716, Training Accuracy: 0.8040444444444445 Validation Accuracy 0.8194\n",
      "Epoch 21 Loss: 0.7150136363079728, Training Accuracy: 0.8154888888888889 Validation Accuracy 0.8292666666666668\n",
      "Epoch 22 Loss: 0.6808916657463466, Training Accuracy: 0.8235777777777777 Validation Accuracy 0.837\n",
      "Epoch 23 Loss: 0.6508939489457792, Training Accuracy: 0.8323999999999999 Validation Accuracy 0.8423999999999999\n",
      "Epoch 24 Loss: 0.6243205725811242, Training Accuracy: 0.8394666666666666 Validation Accuracy 0.8480666666666667\n",
      "Epoch 25 Loss: 0.6010608990186594, Training Accuracy: 0.8454666666666666 Validation Accuracy 0.8515333333333333\n",
      "Epoch 26 Loss: 0.580295153659096, Training Accuracy: 0.8501333333333334 Validation Accuracy 0.8563333333333333\n",
      "Epoch 27 Loss: 0.5618970894464743, Training Accuracy: 0.8545111111111111 Validation Accuracy 0.8608666666666667\n",
      "Epoch 28 Loss: 0.5449437103186324, Training Accuracy: 0.8586 Validation Accuracy 0.8647333333333334\n",
      "Epoch 29 Loss: 0.5298568007014525, Training Accuracy: 0.8624222222222221 Validation Accuracy 0.8670666666666667\n",
      "Epoch 30 Loss: 0.5162120583923808, Training Accuracy: 0.8654222222222223 Validation Accuracy 0.8706\n",
      "Epoch 31 Loss: 0.5032523826757653, Training Accuracy: 0.8688444444444444 Validation Accuracy 0.874\n",
      "Epoch 32 Loss: 0.49188291107047083, Training Accuracy: 0.8714444444444444 Validation Accuracy 0.8755333333333333\n",
      "Epoch 33 Loss: 0.48096168609904344, Training Accuracy: 0.8741555555555555 Validation Accuracy 0.8786666666666667\n",
      "Epoch 34 Loss: 0.4709260789608667, Training Accuracy: 0.8762666666666667 Validation Accuracy 0.8809333333333332\n",
      "Epoch 35 Loss: 0.4617540071682074, Training Accuracy: 0.8782000000000001 Validation Accuracy 0.8826666666666667\n",
      "Epoch 36 Loss: 0.4530063457112767, Training Accuracy: 0.8805333333333335 Validation Accuracy 0.8839333333333333\n",
      "Epoch 37 Loss: 0.44498946408952317, Training Accuracy: 0.8827777777777779 Validation Accuracy 0.8865999999999999\n",
      "Epoch 38 Loss: 0.437413245632596, Training Accuracy: 0.8846222222222223 Validation Accuracy 0.8874666666666667\n",
      "Epoch 39 Loss: 0.4304734862592967, Training Accuracy: 0.8852666666666668 Validation Accuracy 0.8899333333333334\n",
      "Epoch 40 Loss: 0.42376772605526236, Training Accuracy: 0.8869555555555556 Validation Accuracy 0.8920666666666667\n",
      "Epoch 41 Loss: 0.41767655784229374, Training Accuracy: 0.8889111111111109 Validation Accuracy 0.8928666666666666\n",
      "Epoch 42 Loss: 0.4116031352622999, Training Accuracy: 0.8898444444444443 Validation Accuracy 0.8937999999999999\n",
      "Epoch 43 Loss: 0.40634647193670026, Training Accuracy: 0.891 Validation Accuracy 0.8953333333333332\n",
      "Epoch 44 Loss: 0.40110686701012555, Training Accuracy: 0.8923111111111112 Validation Accuracy 0.8959333333333334\n",
      "Epoch 45 Loss: 0.3961131454324997, Training Accuracy: 0.8934888888888889 Validation Accuracy 0.8964000000000001\n",
      "Epoch 46 Loss: 0.3915879067074661, Training Accuracy: 0.8943555555555557 Validation Accuracy 0.8975333333333333\n",
      "Epoch 47 Loss: 0.38716088246544783, Training Accuracy: 0.895 Validation Accuracy 0.898\n",
      "Epoch 48 Loss: 0.38292221058853915, Training Accuracy: 0.8962666666666668 Validation Accuracy 0.9001999999999999\n",
      "Epoch 49 Loss: 0.37895361917385345, Training Accuracy: 0.8970666666666668 Validation Accuracy 0.8998666666666666\n",
      "Epoch 50 Loss: 0.3749913338408937, Training Accuracy: 0.897888888888889 Validation Accuracy 0.9009333333333334\n",
      "Epoch 51 Loss: 0.3713892389525113, Training Accuracy: 0.8986888888888889 Validation Accuracy 0.9010666666666666\n",
      "Epoch 52 Loss: 0.36796697780974447, Training Accuracy: 0.8996666666666666 Validation Accuracy 0.9021333333333332\n",
      "Epoch 53 Loss: 0.3644831968137134, Training Accuracy: 0.9009777777777778 Validation Accuracy 0.9032666666666667\n",
      "Epoch 54 Loss: 0.3615689827819642, Training Accuracy: 0.9013333333333333 Validation Accuracy 0.9038\n",
      "Epoch 55 Loss: 0.35833830540881884, Training Accuracy: 0.9019555555555556 Validation Accuracy 0.9045333333333333\n",
      "Epoch 56 Loss: 0.35521125308714896, Training Accuracy: 0.9026666666666666 Validation Accuracy 0.9046\n",
      "Epoch 57 Loss: 0.35249542304509185, Training Accuracy: 0.9033555555555556 Validation Accuracy 0.9054\n",
      "Epoch 58 Loss: 0.34958487974273594, Training Accuracy: 0.9040888888888888 Validation Accuracy 0.9055333333333333\n",
      "Epoch 59 Loss: 0.346956796309759, Training Accuracy: 0.9046666666666666 Validation Accuracy 0.9063333333333332\n",
      "Epoch 60 Loss: 0.34443408439216894, Training Accuracy: 0.9050222222222222 Validation Accuracy 0.9068666666666666\n",
      "Epoch 61 Loss: 0.34189938892301885, Training Accuracy: 0.9056666666666666 Validation Accuracy 0.9076666666666667\n",
      "Epoch 62 Loss: 0.3395372499256963, Training Accuracy: 0.9061333333333333 Validation Accuracy 0.9079333333333334\n",
      "Epoch 63 Loss: 0.33701864732922127, Training Accuracy: 0.9071777777777777 Validation Accuracy 0.9087999999999999\n",
      "Epoch 64 Loss: 0.3346806172519411, Training Accuracy: 0.9075555555555556 Validation Accuracy 0.9094\n",
      "Epoch 65 Loss: 0.332463201702864, Training Accuracy: 0.9086000000000001 Validation Accuracy 0.9096000000000001\n",
      "Epoch 66 Loss: 0.33031632569496044, Training Accuracy: 0.9084222222222221 Validation Accuracy 0.9109333333333334\n",
      "Epoch 67 Loss: 0.3282094064776886, Training Accuracy: 0.9093555555555556 Validation Accuracy 0.9112666666666667\n",
      "Epoch 68 Loss: 0.325912825841133, Training Accuracy: 0.9095777777777778 Validation Accuracy 0.9116666666666667\n",
      "Epoch 69 Loss: 0.32406898644852816, Training Accuracy: 0.9104666666666666 Validation Accuracy 0.9117333333333334\n",
      "Epoch 70 Loss: 0.3218816956867675, Training Accuracy: 0.9110666666666668 Validation Accuracy 0.9126666666666666\n",
      "Epoch 71 Loss: 0.3199225980111223, Training Accuracy: 0.9110444444444445 Validation Accuracy 0.9126000000000002\n",
      "Epoch 72 Loss: 0.3181006294517878, Training Accuracy: 0.9116222222222222 Validation Accuracy 0.9137333333333334\n",
      "Epoch 73 Loss: 0.31623266828777197, Training Accuracy: 0.9124222222222222 Validation Accuracy 0.9146666666666667\n",
      "Epoch 74 Loss: 0.3144108775991826, Training Accuracy: 0.9127555555555554 Validation Accuracy 0.9152\n",
      "Epoch 75 Loss: 0.31257218435651135, Training Accuracy: 0.9128444444444445 Validation Accuracy 0.9144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Loss: 0.31088402928331066, Training Accuracy: 0.9134666666666669 Validation Accuracy 0.9166666666666666\n",
      "Epoch 77 Loss: 0.3089901610867327, Training Accuracy: 0.9139555555555556 Validation Accuracy 0.9168666666666666\n",
      "Epoch 78 Loss: 0.3074236917834093, Training Accuracy: 0.9141333333333335 Validation Accuracy 0.9164666666666667\n",
      "Epoch 79 Loss: 0.3055665926569909, Training Accuracy: 0.9148444444444445 Validation Accuracy 0.9164666666666667\n",
      "Epoch 80 Loss: 0.30413870740574794, Training Accuracy: 0.9151777777777778 Validation Accuracy 0.918\n",
      "Epoch 81 Loss: 0.30248729794847085, Training Accuracy: 0.9158000000000001 Validation Accuracy 0.9186666666666667\n",
      "Epoch 82 Loss: 0.30077819849296905, Training Accuracy: 0.916 Validation Accuracy 0.9181333333333334\n",
      "Epoch 83 Loss: 0.2991151095721008, Training Accuracy: 0.9169111111111112 Validation Accuracy 0.9185333333333333\n",
      "Epoch 84 Loss: 0.29763806536043313, Training Accuracy: 0.9173555555555556 Validation Accuracy 0.9190666666666667\n",
      "Epoch 85 Loss: 0.2960244666329716, Training Accuracy: 0.9176000000000001 Validation Accuracy 0.9197333333333334\n",
      "Epoch 86 Loss: 0.2944424108983298, Training Accuracy: 0.918 Validation Accuracy 0.9190666666666667\n",
      "Epoch 87 Loss: 0.2930933269675814, Training Accuracy: 0.9186000000000001 Validation Accuracy 0.9197333333333334\n",
      "Epoch 88 Loss: 0.2915445059082725, Training Accuracy: 0.9189111111111112 Validation Accuracy 0.9198666666666667\n",
      "Epoch 89 Loss: 0.29022234062379515, Training Accuracy: 0.9193555555555555 Validation Accuracy 0.921\n",
      "Epoch 90 Loss: 0.28864881123488584, Training Accuracy: 0.9200222222222222 Validation Accuracy 0.9211333333333334\n",
      "Epoch 91 Loss: 0.2873040077159839, Training Accuracy: 0.9201333333333334 Validation Accuracy 0.9216666666666666\n",
      "Epoch 92 Loss: 0.2859411498078936, Training Accuracy: 0.9205111111111111 Validation Accuracy 0.9223333333333333\n",
      "Epoch 93 Loss: 0.2843837919640179, Training Accuracy: 0.9213555555555555 Validation Accuracy 0.9226666666666667\n",
      "Epoch 94 Loss: 0.2830644079626548, Training Accuracy: 0.9213777777777777 Validation Accuracy 0.9225333333333333\n",
      "Epoch 95 Loss: 0.281801273893875, Training Accuracy: 0.9216666666666666 Validation Accuracy 0.9230666666666666\n",
      "Epoch 96 Loss: 0.2804703596022897, Training Accuracy: 0.9221111111111112 Validation Accuracy 0.9236\n",
      "Epoch 97 Loss: 0.2790993649039048, Training Accuracy: 0.9223333333333333 Validation Accuracy 0.9234\n",
      "Epoch 98 Loss: 0.27787512360677485, Training Accuracy: 0.9232444444444445 Validation Accuracy 0.9238\n",
      "Epoch 99 Loss: 0.27656235738273266, Training Accuracy: 0.9234666666666665 Validation Accuracy 0.9240666666666667\n",
      "Epoch 100 Loss: 0.2751750266597581, Training Accuracy: 0.9236666666666666 Validation Accuracy 0.9246666666666666\n",
      "Epoch 101 Loss: 0.27392267628229544, Training Accuracy: 0.9238666666666668 Validation Accuracy 0.9245333333333333\n",
      "Epoch 102 Loss: 0.27272854021176257, Training Accuracy: 0.9241333333333334 Validation Accuracy 0.9245999999999999\n",
      "Epoch 103 Loss: 0.2716465539852447, Training Accuracy: 0.9246222222222222 Validation Accuracy 0.9253333333333333\n",
      "Epoch 104 Loss: 0.2703329395821011, Training Accuracy: 0.924888888888889 Validation Accuracy 0.9255999999999999\n",
      "Epoch 105 Loss: 0.2692452835840973, Training Accuracy: 0.9249333333333334 Validation Accuracy 0.9261333333333334\n",
      "Epoch 106 Loss: 0.2678660405660992, Training Accuracy: 0.9255111111111111 Validation Accuracy 0.9260666666666667\n",
      "Epoch 107 Loss: 0.26683904310845163, Training Accuracy: 0.9258000000000001 Validation Accuracy 0.927\n",
      "Epoch 108 Loss: 0.2655917081622329, Training Accuracy: 0.925888888888889 Validation Accuracy 0.9270666666666667\n",
      "Epoch 109 Loss: 0.2645664437814363, Training Accuracy: 0.9263777777777777 Validation Accuracy 0.9277333333333333\n",
      "Epoch 110 Loss: 0.2633860994338761, Training Accuracy: 0.9266888888888889 Validation Accuracy 0.9278666666666665\n",
      "Epoch 111 Loss: 0.2622119353769561, Training Accuracy: 0.9268000000000001 Validation Accuracy 0.9276666666666668\n",
      "Epoch 112 Loss: 0.26115408561835884, Training Accuracy: 0.9270666666666667 Validation Accuracy 0.9282\n",
      "Epoch 113 Loss: 0.2601029697873703, Training Accuracy: 0.9273111111111111 Validation Accuracy 0.9283999999999999\n",
      "Epoch 114 Loss: 0.2589139686554585, Training Accuracy: 0.9275333333333333 Validation Accuracy 0.9286666666666668\n",
      "Epoch 115 Loss: 0.2579048660274679, Training Accuracy: 0.9278666666666666 Validation Accuracy 0.9287333333333333\n",
      "Epoch 116 Loss: 0.2569043898509904, Training Accuracy: 0.9284000000000001 Validation Accuracy 0.929\n",
      "Epoch 117 Loss: 0.2558618272062768, Training Accuracy: 0.9285555555555557 Validation Accuracy 0.9292666666666667\n",
      "Epoch 118 Loss: 0.25464138794221164, Training Accuracy: 0.9286666666666666 Validation Accuracy 0.9296\n",
      "Epoch 119 Loss: 0.2538972167718894, Training Accuracy: 0.9289777777777778 Validation Accuracy 0.9298666666666666\n",
      "Epoch 120 Loss: 0.25260419830389125, Training Accuracy: 0.9292888888888889 Validation Accuracy 0.9298\n",
      "Epoch 121 Loss: 0.25182488106457246, Training Accuracy: 0.9293777777777779 Validation Accuracy 0.9303333333333333\n",
      "Epoch 122 Loss: 0.25079745790629926, Training Accuracy: 0.9294222222222223 Validation Accuracy 0.9308\n",
      "Epoch 123 Loss: 0.24969571643053642, Training Accuracy: 0.929711111111111 Validation Accuracy 0.9311333333333335\n",
      "Epoch 124 Loss: 0.24875637981953788, Training Accuracy: 0.9303555555555555 Validation Accuracy 0.9313333333333333\n",
      "Epoch 125 Loss: 0.24798416873535545, Training Accuracy: 0.9304444444444443 Validation Accuracy 0.9316666666666668\n",
      "Epoch 126 Loss: 0.24683641962132613, Training Accuracy: 0.9306444444444445 Validation Accuracy 0.931\n",
      "Epoch 127 Loss: 0.2460084327568101, Training Accuracy: 0.9311555555555555 Validation Accuracy 0.9322666666666667\n",
      "Epoch 128 Loss: 0.24499766516455726, Training Accuracy: 0.9311333333333333 Validation Accuracy 0.9321333333333334\n",
      "Epoch 129 Loss: 0.24406639316256556, Training Accuracy: 0.9314444444444445 Validation Accuracy 0.9323999999999999\n",
      "Epoch 130 Loss: 0.24316181513986895, Training Accuracy: 0.9317999999999999 Validation Accuracy 0.9325333333333333\n",
      "Epoch 131 Loss: 0.24231954691254134, Training Accuracy: 0.9321555555555557 Validation Accuracy 0.9328666666666667\n",
      "Epoch 132 Loss: 0.24130902815288155, Training Accuracy: 0.9324666666666667 Validation Accuracy 0.9322666666666667\n",
      "Epoch 133 Loss: 0.24054073801591835, Training Accuracy: 0.9326666666666666 Validation Accuracy 0.9327333333333333\n",
      "Epoch 134 Loss: 0.23959802907548577, Training Accuracy: 0.9327555555555554 Validation Accuracy 0.9330666666666666\n",
      "Epoch 135 Loss: 0.23869367984305523, Training Accuracy: 0.9337777777777777 Validation Accuracy 0.9332666666666667\n",
      "Epoch 136 Loss: 0.2379545704157753, Training Accuracy: 0.9334000000000001 Validation Accuracy 0.9331333333333334\n",
      "Epoch 137 Loss: 0.236991014330151, Training Accuracy: 0.9335777777777778 Validation Accuracy 0.9334\n",
      "Epoch 138 Loss: 0.23614571137313756, Training Accuracy: 0.9340888888888889 Validation Accuracy 0.9335333333333333\n",
      "Epoch 139 Loss: 0.23537263671953185, Training Accuracy: 0.9342444444444445 Validation Accuracy 0.9341999999999999\n",
      "Epoch 140 Loss: 0.23446883262111526, Training Accuracy: 0.9342444444444445 Validation Accuracy 0.9335333333333333\n",
      "Epoch 141 Loss: 0.23377061274416686, Training Accuracy: 0.9346000000000001 Validation Accuracy 0.9340666666666667\n",
      "Epoch 142 Loss: 0.23286935404474313, Training Accuracy: 0.9347777777777778 Validation Accuracy 0.9348666666666665\n",
      "Epoch 143 Loss: 0.23201234046222532, Training Accuracy: 0.9348222222222222 Validation Accuracy 0.9344666666666667\n",
      "Epoch 144 Loss: 0.2311858652416174, Training Accuracy: 0.935511111111111 Validation Accuracy 0.9351333333333334\n",
      "Epoch 145 Loss: 0.23050636329181012, Training Accuracy: 0.9353111111111111 Validation Accuracy 0.9350666666666666\n",
      "Epoch 146 Loss: 0.22959024636308747, Training Accuracy: 0.9353555555555555 Validation Accuracy 0.9348\n",
      "Epoch 147 Loss: 0.22898767944453025, Training Accuracy: 0.935711111111111 Validation Accuracy 0.9351333333333334\n",
      "Epoch 148 Loss: 0.22808165343437437, Training Accuracy: 0.9360666666666666 Validation Accuracy 0.9352666666666667\n",
      "Epoch 149 Loss: 0.22740253925349774, Training Accuracy: 0.9362222222222223 Validation Accuracy 0.9360666666666666\n",
      "Epoch 150 Loss: 0.22662246724232077, Training Accuracy: 0.9363333333333334 Validation Accuracy 0.9358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151 Loss: 0.22582931134946102, Training Accuracy: 0.9367333333333333 Validation Accuracy 0.9364\n",
      "Epoch 152 Loss: 0.22519260652445788, Training Accuracy: 0.9371333333333334 Validation Accuracy 0.9363333333333332\n",
      "Epoch 153 Loss: 0.2243214117411512, Training Accuracy: 0.9366888888888889 Validation Accuracy 0.9364000000000001\n",
      "Epoch 154 Loss: 0.2236076340244839, Training Accuracy: 0.9372222222222223 Validation Accuracy 0.9365999999999999\n",
      "Epoch 155 Loss: 0.22288090710493957, Training Accuracy: 0.9376000000000001 Validation Accuracy 0.9366666666666668\n",
      "Epoch 156 Loss: 0.22223897881299906, Training Accuracy: 0.9375333333333334 Validation Accuracy 0.9368\n",
      "Epoch 157 Loss: 0.2214945865019514, Training Accuracy: 0.9376444444444445 Validation Accuracy 0.9369333333333334\n",
      "Epoch 158 Loss: 0.22082181763625788, Training Accuracy: 0.9380222222222222 Validation Accuracy 0.9376\n",
      "Epoch 159 Loss: 0.21998964754777003, Training Accuracy: 0.938111111111111 Validation Accuracy 0.9378666666666665\n",
      "Epoch 160 Loss: 0.2192873780616619, Training Accuracy: 0.9385111111111111 Validation Accuracy 0.9377333333333333\n",
      "Epoch 161 Loss: 0.21870242990594613, Training Accuracy: 0.9386000000000001 Validation Accuracy 0.938\n",
      "Epoch 162 Loss: 0.21798534945705797, Training Accuracy: 0.9388000000000001 Validation Accuracy 0.9377333333333333\n",
      "Epoch 163 Loss: 0.21727940174295682, Training Accuracy: 0.939111111111111 Validation Accuracy 0.9384\n",
      "Epoch 164 Loss: 0.21668685733556625, Training Accuracy: 0.9387777777777778 Validation Accuracy 0.9386666666666668\n",
      "Epoch 165 Loss: 0.2159896285842464, Training Accuracy: 0.9393999999999999 Validation Accuracy 0.9387333333333333\n",
      "Epoch 166 Loss: 0.21531575155543378, Training Accuracy: 0.9395111111111111 Validation Accuracy 0.9390666666666667\n",
      "Epoch 167 Loss: 0.2146279818574898, Training Accuracy: 0.9398222222222222 Validation Accuracy 0.9388000000000001\n",
      "Epoch 168 Loss: 0.2140306947485722, Training Accuracy: 0.9399333333333333 Validation Accuracy 0.9391999999999998\n",
      "Epoch 169 Loss: 0.213250595988045, Training Accuracy: 0.9399333333333334 Validation Accuracy 0.9395999999999999\n",
      "Epoch 170 Loss: 0.21269158901818075, Training Accuracy: 0.9401999999999999 Validation Accuracy 0.9393333333333334\n",
      "Epoch 171 Loss: 0.21198018084576672, Training Accuracy: 0.9402444444444445 Validation Accuracy 0.9391999999999999\n",
      "Epoch 172 Loss: 0.21134169988803417, Training Accuracy: 0.9407555555555555 Validation Accuracy 0.9396666666666667\n",
      "Epoch 173 Loss: 0.21073528459130306, Training Accuracy: 0.9410222222222222 Validation Accuracy 0.9398666666666665\n",
      "Epoch 174 Loss: 0.21011030501241126, Training Accuracy: 0.940711111111111 Validation Accuracy 0.9393333333333334\n",
      "Epoch 175 Loss: 0.20954673822580833, Training Accuracy: 0.9414444444444443 Validation Accuracy 0.9398666666666666\n",
      "Epoch 176 Loss: 0.20891929571454687, Training Accuracy: 0.9412888888888887 Validation Accuracy 0.9396666666666668\n",
      "Epoch 177 Loss: 0.2082490975985846, Training Accuracy: 0.9414666666666667 Validation Accuracy 0.9401333333333334\n",
      "Epoch 178 Loss: 0.207668956779959, Training Accuracy: 0.941711111111111 Validation Accuracy 0.9402666666666667\n",
      "Epoch 179 Loss: 0.207082298191907, Training Accuracy: 0.9421777777777778 Validation Accuracy 0.9402666666666667\n",
      "Epoch 180 Loss: 0.20647023914771054, Training Accuracy: 0.9418222222222222 Validation Accuracy 0.9405333333333333\n",
      "Epoch 181 Loss: 0.20591876363600659, Training Accuracy: 0.9423999999999999 Validation Accuracy 0.9408666666666666\n",
      "Epoch 182 Loss: 0.205238787897049, Training Accuracy: 0.9421777777777779 Validation Accuracy 0.9409333333333333\n",
      "Epoch 183 Loss: 0.20468280620663962, Training Accuracy: 0.9421999999999998 Validation Accuracy 0.941\n",
      "Epoch 184 Loss: 0.20413439784868542, Training Accuracy: 0.9426888888888889 Validation Accuracy 0.9412666666666667\n",
      "Epoch 185 Loss: 0.20365765631600857, Training Accuracy: 0.9426000000000001 Validation Accuracy 0.9411333333333334\n",
      "Epoch 186 Loss: 0.2029786350616797, Training Accuracy: 0.9427777777777778 Validation Accuracy 0.9408\n",
      "Epoch 187 Loss: 0.20238478653795658, Training Accuracy: 0.9430888888888888 Validation Accuracy 0.9416666666666667\n",
      "Epoch 188 Loss: 0.2018627830205445, Training Accuracy: 0.9429555555555555 Validation Accuracy 0.9414666666666667\n",
      "Epoch 189 Loss: 0.20127482655749843, Training Accuracy: 0.9431333333333334 Validation Accuracy 0.9418666666666666\n",
      "Epoch 190 Loss: 0.2006874511890545, Training Accuracy: 0.9433111111111111 Validation Accuracy 0.942\n",
      "Epoch 191 Loss: 0.20016279183599356, Training Accuracy: 0.9437111111111111 Validation Accuracy 0.9427333333333333\n",
      "Epoch 192 Loss: 0.19965382585817498, Training Accuracy: 0.9436888888888887 Validation Accuracy 0.9420666666666666\n",
      "Epoch 193 Loss: 0.19908887709851306, Training Accuracy: 0.9438888888888889 Validation Accuracy 0.943\n",
      "Epoch 194 Loss: 0.19848107920994335, Training Accuracy: 0.9441333333333332 Validation Accuracy 0.9427333333333332\n",
      "Epoch 195 Loss: 0.197924184605304, Training Accuracy: 0.9441777777777779 Validation Accuracy 0.943\n",
      "Epoch 196 Loss: 0.1974112604738545, Training Accuracy: 0.9442444444444446 Validation Accuracy 0.9424666666666667\n",
      "Epoch 197 Loss: 0.19686239207985637, Training Accuracy: 0.9443777777777778 Validation Accuracy 0.9431333333333334\n",
      "Epoch 198 Loss: 0.1963654407358218, Training Accuracy: 0.9446444444444445 Validation Accuracy 0.9438666666666667\n",
      "Epoch 199 Loss: 0.19589383579525868, Training Accuracy: 0.9447333333333333 Validation Accuracy 0.9430666666666666\n",
      "Epoch 200 Loss: 0.19527768495588066, Training Accuracy: 0.9447111111111111 Validation Accuracy 0.9434000000000001\n",
      "Epoch 201 Loss: 0.1949162096904084, Training Accuracy: 0.9447555555555555 Validation Accuracy 0.9434\n",
      "Epoch 202 Loss: 0.1942869969582171, Training Accuracy: 0.9448444444444443 Validation Accuracy 0.9433333333333332\n",
      "Epoch 203 Loss: 0.19375889303555152, Training Accuracy: 0.9452666666666666 Validation Accuracy 0.9431333333333334\n",
      "Epoch 204 Loss: 0.19318263440962327, Training Accuracy: 0.9454666666666667 Validation Accuracy 0.9433333333333332\n",
      "Epoch 205 Loss: 0.19275700075822835, Training Accuracy: 0.945111111111111 Validation Accuracy 0.9438666666666665\n",
      "Epoch 206 Loss: 0.19208911234273163, Training Accuracy: 0.9457333333333333 Validation Accuracy 0.9429333333333334\n",
      "Epoch 207 Loss: 0.19173692862189215, Training Accuracy: 0.9454222222222222 Validation Accuracy 0.9444666666666667\n",
      "Epoch 208 Loss: 0.19126998248654054, Training Accuracy: 0.945911111111111 Validation Accuracy 0.9441333333333334\n",
      "Epoch 209 Loss: 0.19081946404886388, Training Accuracy: 0.9458444444444445 Validation Accuracy 0.9443333333333332\n",
      "Epoch 210 Loss: 0.19036967941301175, Training Accuracy: 0.9459555555555555 Validation Accuracy 0.9442666666666667\n",
      "Epoch 211 Loss: 0.18979743466042082, Training Accuracy: 0.9460444444444444 Validation Accuracy 0.9442\n",
      "Epoch 212 Loss: 0.1893502589173746, Training Accuracy: 0.9463777777777778 Validation Accuracy 0.9444666666666667\n",
      "Epoch 213 Loss: 0.18877316610015846, Training Accuracy: 0.9464888888888889 Validation Accuracy 0.9442\n",
      "Epoch 214 Loss: 0.18838960239731128, Training Accuracy: 0.9466222222222223 Validation Accuracy 0.9446\n",
      "Epoch 215 Loss: 0.18793511459413367, Training Accuracy: 0.9468888888888889 Validation Accuracy 0.9448\n",
      "Epoch 216 Loss: 0.18742541767424256, Training Accuracy: 0.9467111111111111 Validation Accuracy 0.9451333333333334\n",
      "Epoch 217 Loss: 0.18685332496174462, Training Accuracy: 0.9466444444444445 Validation Accuracy 0.9446666666666668\n",
      "Epoch 218 Loss: 0.18653041009953034, Training Accuracy: 0.9472222222222223 Validation Accuracy 0.9446666666666668\n",
      "Epoch 219 Loss: 0.1859434143637503, Training Accuracy: 0.9469777777777778 Validation Accuracy 0.9448000000000001\n",
      "Epoch 220 Loss: 0.18560907709678118, Training Accuracy: 0.946911111111111 Validation Accuracy 0.9449333333333333\n",
      "Epoch 221 Loss: 0.18512199178735436, Training Accuracy: 0.9471777777777778 Validation Accuracy 0.9454\n",
      "Epoch 222 Loss: 0.18461360380086833, Training Accuracy: 0.9473555555555555 Validation Accuracy 0.9453333333333334\n",
      "Epoch 223 Loss: 0.1840802494187424, Training Accuracy: 0.9471333333333332 Validation Accuracy 0.9454666666666667\n",
      "Epoch 224 Loss: 0.18375762162580808, Training Accuracy: 0.9479555555555554 Validation Accuracy 0.9454666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 Loss: 0.1832786064830493, Training Accuracy: 0.9477111111111111 Validation Accuracy 0.9456666666666667\n",
      "Epoch 226 Loss: 0.1828590869661514, Training Accuracy: 0.9483777777777778 Validation Accuracy 0.9452\n",
      "Epoch 227 Loss: 0.18244882115784664, Training Accuracy: 0.9481777777777778 Validation Accuracy 0.9456666666666667\n",
      "Epoch 228 Loss: 0.181937280745805, Training Accuracy: 0.9483999999999999 Validation Accuracy 0.9459333333333333\n",
      "Epoch 229 Loss: 0.18150302460629306, Training Accuracy: 0.9486888888888889 Validation Accuracy 0.946\n",
      "Epoch 230 Loss: 0.18106593899704498, Training Accuracy: 0.9488888888888889 Validation Accuracy 0.9460666666666666\n",
      "Epoch 231 Loss: 0.18066933878706376, Training Accuracy: 0.9486666666666667 Validation Accuracy 0.9461333333333333\n",
      "Epoch 232 Loss: 0.18025809275578855, Training Accuracy: 0.9492444444444446 Validation Accuracy 0.9462\n",
      "Epoch 233 Loss: 0.17981268605793635, Training Accuracy: 0.949288888888889 Validation Accuracy 0.9459333333333333\n",
      "Epoch 234 Loss: 0.17944190208573907, Training Accuracy: 0.9493111111111111 Validation Accuracy 0.9466\n",
      "Epoch 235 Loss: 0.1788495119271241, Training Accuracy: 0.949288888888889 Validation Accuracy 0.9460666666666667\n",
      "Epoch 236 Loss: 0.17853700379610654, Training Accuracy: 0.9495111111111111 Validation Accuracy 0.9468666666666666\n",
      "Epoch 237 Loss: 0.17815020543039425, Training Accuracy: 0.9496222222222223 Validation Accuracy 0.9464000000000001\n",
      "Epoch 238 Loss: 0.1776864289430686, Training Accuracy: 0.9496666666666667 Validation Accuracy 0.9468666666666666\n",
      "Epoch 239 Loss: 0.17731191970294496, Training Accuracy: 0.9501999999999999 Validation Accuracy 0.9466\n",
      "Epoch 240 Loss: 0.17689613353196304, Training Accuracy: 0.9498222222222221 Validation Accuracy 0.9468666666666666\n",
      "Epoch 241 Loss: 0.17648422745623468, Training Accuracy: 0.949888888888889 Validation Accuracy 0.9466666666666668\n",
      "Epoch 242 Loss: 0.17599931896280765, Training Accuracy: 0.950288888888889 Validation Accuracy 0.9472666666666667\n",
      "Epoch 243 Loss: 0.17566793424941887, Training Accuracy: 0.9502444444444443 Validation Accuracy 0.947\n",
      "Epoch 244 Loss: 0.17522688041804546, Training Accuracy: 0.9507777777777778 Validation Accuracy 0.9471333333333333\n",
      "Epoch 245 Loss: 0.17487043321826032, Training Accuracy: 0.9507555555555555 Validation Accuracy 0.9476666666666667\n",
      "Epoch 246 Loss: 0.17445552093804825, Training Accuracy: 0.9505111111111111 Validation Accuracy 0.9474\n",
      "Epoch 247 Loss: 0.17405752039656705, Training Accuracy: 0.9508888888888888 Validation Accuracy 0.9475333333333334\n",
      "Epoch 248 Loss: 0.17359058905384245, Training Accuracy: 0.9508888888888888 Validation Accuracy 0.9471333333333335\n",
      "Epoch 249 Loss: 0.1732893460481614, Training Accuracy: 0.9510666666666666 Validation Accuracy 0.9473333333333334\n",
      "Epoch 250 Loss: 0.17286297529440253, Training Accuracy: 0.9510444444444444 Validation Accuracy 0.9479333333333334\n",
      "Epoch 251 Loss: 0.17245529666663323, Training Accuracy: 0.9514222222222222 Validation Accuracy 0.948\n",
      "Epoch 252 Loss: 0.17204941465789875, Training Accuracy: 0.9518222222222222 Validation Accuracy 0.9471999999999999\n",
      "Epoch 253 Loss: 0.17174589749612182, Training Accuracy: 0.9516444444444445 Validation Accuracy 0.9475333333333334\n",
      "Epoch 254 Loss: 0.17134064490050127, Training Accuracy: 0.9513777777777778 Validation Accuracy 0.9477333333333333\n",
      "Epoch 255 Loss: 0.17094718807543668, Training Accuracy: 0.9520444444444444 Validation Accuracy 0.9476666666666668\n",
      "Epoch 256 Loss: 0.17054805332983017, Training Accuracy: 0.9522000000000002 Validation Accuracy 0.9482666666666667\n",
      "Epoch 257 Loss: 0.17012900469771092, Training Accuracy: 0.9523111111111111 Validation Accuracy 0.9484666666666667\n",
      "Epoch 258 Loss: 0.16983769599948545, Training Accuracy: 0.9525555555555555 Validation Accuracy 0.9482\n",
      "Epoch 259 Loss: 0.16943860339321368, Training Accuracy: 0.9521999999999999 Validation Accuracy 0.9482\n",
      "Epoch 260 Loss: 0.16911982316947086, Training Accuracy: 0.9523111111111111 Validation Accuracy 0.9482666666666667\n",
      "Epoch 261 Loss: 0.1686484783794985, Training Accuracy: 0.9526666666666667 Validation Accuracy 0.9484\n",
      "Epoch 262 Loss: 0.16837754458158527, Training Accuracy: 0.9524222222222222 Validation Accuracy 0.9482666666666667\n",
      "Epoch 263 Loss: 0.16794583963656642, Training Accuracy: 0.9527555555555555 Validation Accuracy 0.9485333333333333\n",
      "Epoch 264 Loss: 0.1676130333061669, Training Accuracy: 0.953111111111111 Validation Accuracy 0.9487333333333333\n",
      "Epoch 265 Loss: 0.16732956941698676, Training Accuracy: 0.953111111111111 Validation Accuracy 0.9483333333333334\n",
      "Epoch 266 Loss: 0.16682701480528794, Training Accuracy: 0.9532666666666667 Validation Accuracy 0.9482666666666667\n",
      "Epoch 267 Loss: 0.1664910823311639, Training Accuracy: 0.9534888888888887 Validation Accuracy 0.9487333333333333\n",
      "Epoch 268 Loss: 0.16622420451453626, Training Accuracy: 0.9533333333333334 Validation Accuracy 0.9486\n",
      "Epoch 269 Loss: 0.16574958695361525, Training Accuracy: 0.9531999999999999 Validation Accuracy 0.9488\n",
      "Epoch 270 Loss: 0.1654239629457147, Training Accuracy: 0.9540666666666666 Validation Accuracy 0.9485333333333333\n",
      "Epoch 271 Loss: 0.1651496220815072, Training Accuracy: 0.9538888888888888 Validation Accuracy 0.9492666666666667\n",
      "Epoch 272 Loss: 0.16477588888666128, Training Accuracy: 0.9539111111111112 Validation Accuracy 0.9485999999999999\n",
      "Epoch 273 Loss: 0.16440557050921328, Training Accuracy: 0.9542666666666667 Validation Accuracy 0.9491333333333333\n",
      "Epoch 274 Loss: 0.16406396740214096, Training Accuracy: 0.9540888888888888 Validation Accuracy 0.949\n",
      "Epoch 275 Loss: 0.1638198483787229, Training Accuracy: 0.9541555555555554 Validation Accuracy 0.9489333333333334\n",
      "Epoch 276 Loss: 0.16343794013233998, Training Accuracy: 0.9543999999999999 Validation Accuracy 0.9492666666666667\n",
      "Epoch 277 Loss: 0.1631052459149487, Training Accuracy: 0.9545111111111111 Validation Accuracy 0.9491333333333332\n",
      "Epoch 278 Loss: 0.16275343310651072, Training Accuracy: 0.9545333333333333 Validation Accuracy 0.949\n",
      "Epoch 279 Loss: 0.16237055766755928, Training Accuracy: 0.9546000000000001 Validation Accuracy 0.9493333333333333\n",
      "Epoch 280 Loss: 0.1620591878505709, Training Accuracy: 0.9549111111111112 Validation Accuracy 0.9487333333333333\n",
      "Epoch 281 Loss: 0.16175821250840094, Training Accuracy: 0.9550444444444444 Validation Accuracy 0.9488\n",
      "Epoch 282 Loss: 0.1613726286531492, Training Accuracy: 0.9549555555555554 Validation Accuracy 0.9494\n",
      "Epoch 283 Loss: 0.1610877094793059, Training Accuracy: 0.9552000000000002 Validation Accuracy 0.949\n",
      "Epoch 284 Loss: 0.16077855019415233, Training Accuracy: 0.9551333333333332 Validation Accuracy 0.9493333333333333\n",
      "Epoch 285 Loss: 0.16030970269614775, Training Accuracy: 0.9551333333333334 Validation Accuracy 0.9495333333333335\n",
      "Epoch 286 Loss: 0.1600915434766356, Training Accuracy: 0.9552666666666666 Validation Accuracy 0.9495999999999999\n",
      "Epoch 287 Loss: 0.15964977437200487, Training Accuracy: 0.9553777777777778 Validation Accuracy 0.9491999999999999\n",
      "Epoch 288 Loss: 0.15945439374231263, Training Accuracy: 0.9553777777777778 Validation Accuracy 0.9496000000000001\n",
      "Epoch 289 Loss: 0.15905934458925913, Training Accuracy: 0.9555555555555554 Validation Accuracy 0.9496666666666667\n",
      "Epoch 290 Loss: 0.15878759547963586, Training Accuracy: 0.955511111111111 Validation Accuracy 0.9494666666666666\n",
      "Epoch 291 Loss: 0.1584631788904981, Training Accuracy: 0.9556444444444443 Validation Accuracy 0.9496666666666668\n",
      "Epoch 292 Loss: 0.15813174795608534, Training Accuracy: 0.9559333333333332 Validation Accuracy 0.9498\n",
      "Epoch 293 Loss: 0.15777926112665785, Training Accuracy: 0.9557777777777777 Validation Accuracy 0.9500000000000001\n",
      "Epoch 294 Loss: 0.15760320464922026, Training Accuracy: 0.9560888888888888 Validation Accuracy 0.9496666666666667\n",
      "Epoch 295 Loss: 0.15718153281231026, Training Accuracy: 0.9562222222222222 Validation Accuracy 0.9500666666666666\n",
      "Epoch 296 Loss: 0.15691254631877263, Training Accuracy: 0.956511111111111 Validation Accuracy 0.9499333333333334\n",
      "Epoch 297 Loss: 0.15646826874704103, Training Accuracy: 0.956311111111111 Validation Accuracy 0.9503333333333334\n",
      "Epoch 298 Loss: 0.1563275167717755, Training Accuracy: 0.9566444444444445 Validation Accuracy 0.9500666666666666\n",
      "Epoch 299 Loss: 0.15594895068044912, Training Accuracy: 0.956511111111111 Validation Accuracy 0.9499333333333333\n"
     ]
    }
   ],
   "source": [
    "def run_epoch(X, y, n, test=False):\n",
    "    '''Computes loss and accuracy for one epoch using mini-batch gradient descent. \n",
    "    '''\n",
    "    \n",
    "    #shuffle datasets\n",
    "    index = np.arange(n)\n",
    "    np.random.shuffle(index)\n",
    "    X = X[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    #divide dataset into batches \n",
    "    batch_count = n / batch_size\n",
    "    X_batches = np.array_split(X, batch_count)\n",
    "    y_batches = np.array_split(y, batch_count)\n",
    "    \n",
    "    #run gradient descent on each batch and save loss and accuracies \n",
    "    losses = []\n",
    "    accs = []\n",
    "    for i in range(len(X_batches)):\n",
    "        loss, acc = gradient_descent(X_batches[i], y_batches[i], forward_only=test)\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "        \n",
    "    #return the average loss and accuracy of the batch \n",
    "    return np.average(losses), np.average(accs)\n",
    "\n",
    "num_epochs = 300\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "#run the test, training and validation sets through the neural network and save accuracies and loss \n",
    "for i in range(num_epochs):\n",
    "    loss, acc = run_epoch(X_train,Y_train,m)\n",
    "    loss_, acc_ = run_epoch(X_val, Y_val,m_val, True) #no backward propagation on val set.\n",
    "    test_loss, test_acc = run_epoch(X_test, Y_test, m_test, True) #no backward propagation on test set\n",
    "    train_losses.append(loss)\n",
    "    test_losses.append(test_loss)\n",
    "    val_losses.append(loss_)\n",
    "    val_accuracies.append(acc_)\n",
    "    train_accuracies.append(acc)\n",
    "    print(f\"Epoch {i} Loss: {loss}, Training Accuracy: {acc} Validation Accuracy {acc_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9f33839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  95.1 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbae0220c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss_, test_accuracy_ = gradient_descent(X_test, Y_test, True)\n",
    "\n",
    "print('Test Accuracy: ', test_accuracy_ *100, '%')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(300)),train_losses)\n",
    "plt.plot(list(range(300)),val_losses)\n",
    "plt.plot(list(range(300)),test_losses)\n",
    "\n",
    "plt.xlabel(\"Loss/Cost\")\n",
    "plt.ylabel(\"No. of Epochs\")\n",
    "plt.title(\"Figure 1. Neural Network Performance During Training\")\n",
    "plt.legend(['Training Set','validation Set', 'Test Set'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
