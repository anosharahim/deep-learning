{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c474a39c",
   "metadata": {},
   "source": [
    "# Building a Neural Network from Scratch\n",
    "\n",
    "## Requirements \n",
    "\n",
    "- A working fully-connected deep neural network from scratch using only numpy.\n",
    "- Includes dense layers, activations, optimizers, loss functions and sigmoid or softmax in case of classification. \n",
    "- Runtime and results on a public dataset.\n",
    "- Documented code that includes brief summary, technical details, and results. \n",
    "\n",
    "## Extensions \n",
    "\n",
    "A comparison of the modelâ€™s runtime and performance with/without each component:\n",
    "- More than 1 optimizer - SGD, Momentum, RMSProp, Adam etc.\n",
    "- Regularization - L2/weight decay, dropout, possibly augmentations if image data etc.\n",
    "- Results on more than 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "#load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "x = mnist.data\n",
    "y = mnist.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c07c9",
   "metadata": {},
   "source": [
    "Forward pass on a single example:\n",
    "$\\hat{y}=\\sigma(w^Tx+b)$ \\\n",
    "Sigmoid Function:\n",
    "$\\sigma = \\frac{1}{1+e^{-z}}$ \\\n",
    "In backpropagation, we need to compute: $\\frac{\\partial L}{\\partial w_j}$ i.e. we need to know how the cost changes with respect to each component of the weight matrix. In other words, we need to know how sensitive the cost function is to each of the components of the weight matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668ae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define the sigmoid activation function \n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def cross_entropy(Y, Y_hat):\n",
    "    b = Y.shape[1]\n",
    "    L = -(1./b) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
    "    return L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#one input layer\n",
    "#two hidden layers\n",
    "#one output layer\n",
    "\n",
    "#MNIST -- 70000 images of 28x28 pixels and 10 classes\n",
    "#stochastic gradient descent \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
